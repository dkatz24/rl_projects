{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import tensorflow as tf\n",
    "import retro\n",
    "\n",
    "import numpy as np\n",
    "import random as r\n",
    "\n",
    "from skimage import transform # to preprocess game frames\n",
    "from skimage.color import rgb2gray # to convert game frames into grayscale\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from collections import deque # to create ordered collections of frames\n",
    "\n",
    "import warnings # ignore warning messages from skiimage during training\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frame Size:  Box(210, 160, 3)\n",
      "Action Space:  6  moves\n",
      "[[1 0 0 0 0 0]\n",
      " [0 1 0 0 0 0]\n",
      " [0 0 1 0 0 0]\n",
      " [0 0 0 1 0 0]\n",
      " [0 0 0 0 1 0]\n",
      " [0 0 0 0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('SpaceInvaders-v0') # make env\n",
    "\n",
    "print(\"Frame Size: \", env.observation_space)\n",
    "print(\"Action Space: \", env.action_space.n, \" moves\")\n",
    "\n",
    "H = 110 # height of preprocessed image\n",
    "W = 84 # width of preprocessed image\n",
    "\n",
    "moves = np.array(np.identity(env.action_space.n, dtype=int).tolist())\n",
    "\n",
    "print(moves)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input size (210, 160, 3)\n"
     ]
    }
   ],
   "source": [
    "for i_episode in range(1):\n",
    "    observation = env.reset()\n",
    "    print(\"input size\", observation.shape)\n",
    "#     for t in range(1):\n",
    "#         env.render()\n",
    "#         print(observation)\n",
    "#         action = env.action_space.sample()\n",
    "#         observation, reward, done, info = env.step(action)\n",
    "#         if done:\n",
    "#             print(\"Episode finished after {} timesteps\".format(t+1))\n",
    "#             break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "STATE_SIZE = [110, 84, 4] # stack dimensions\n",
    "STACK_SIZE = 4 # \"phi length\" - how many frames we consolidate into one training example\n",
    "ACTION_SIZE = env.action_space.n # 6 possible moves\n",
    "LEARNING_RATE = 2.5e-4\n",
    "\n",
    "# training\n",
    "NUM_GAMES = 250 # how many games we will play for training\n",
    "MAX_STEPS = 50000 # max number of moves we'll make each game\n",
    "HIDDEN_UNITS = 512\n",
    "BATCH_SIZE = 1 # num of states we include for each weight update in gradient descent\n",
    "TRAINING = True # when you just want agent to play\n",
    "ENV_RENDER = True # when you want to see env\n",
    "\n",
    "# explore params\n",
    "EXP_START = 1.0\n",
    "EXP_STOP = 0.01\n",
    "DECAY = 1e-5\n",
    "\n",
    "# q le table\n",
    "GAMMA = 0.001 # discount for future rewards in bellman equation\n",
    "\n",
    "# memory\n",
    "LOAD_MEM = 64 # num experiences to be stored in memory during initialization\n",
    "MEMORY_SIZE = int(1e6) # max num experiences stored in memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess raw input from game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def state_to_frame(state):\n",
    "    \n",
    "#     print(\"Input Size: \", state.shape)\n",
    "    \n",
    "    gray_doe = rgb2gray(state)\n",
    "    crop_doe = gray_doe[6:-12, 4:-12]\n",
    "    normalize_doe = crop_doe / 255.0\n",
    "    frame = transform.resize(normalize_doe, [H, W])\n",
    "    \n",
    "#     print(\"Output Size: \", frame.shape)\n",
    "    \n",
    "    return  frame # Shape = 108 height, 84 width | vals = [0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stack_frames(stack_o_frames, state, is_new_game):\n",
    "    \n",
    "    frame = state_to_frame(state)\n",
    "    \n",
    "    if is_new_game: # if new game, clear stack from last game\n",
    "        \n",
    "        for i in range(STACK_SIZE): # in new game, start with copies of OG frame\n",
    "            stack_o_frames.append(frame)\n",
    "            \n",
    "#         print(\"Stack Size: \", len(stack_o_frames))  \n",
    "#         print(\"State Size: \", state.shape)\n",
    "        \n",
    "    else: # if we already have stack, append latest frame\n",
    "        \n",
    "        stack_o_frames.append(frame) # deque automatically removes oldest frame\n",
    "        \n",
    "    state = np.stack(stack_o_frames, axis=2) # input to neural net, shape = (108, 84, 4)\n",
    "       \n",
    "#     print(\"Snapshot: \", state[:5])\n",
    "        \n",
    "    return stack_o_frames, state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN:\n",
    "    def __init__(self, name=\"DQN\"):\n",
    "        \n",
    "        with tf.variable_scope(name): # create model skeleton, using placeholders for data to be fed in\n",
    "            \n",
    "            self.inputs_ = tf.placeholder(dtype=tf.float32, \n",
    "                                          shape=[None, *STATE_SIZE], \n",
    "                                          # *STATE_SIZE means we take each element of STATE_SIZE and input sequentially\n",
    "                                          # ie. [None, 110, 84, 4] instead of [None, [110, 84, 4]]\n",
    "                                          name=\"inputs\")\n",
    "            \n",
    "            self.inputs_shape = self.inputs_.get_shape()\n",
    "            \n",
    "            self.actions_ = tf.placeholder(dtype=tf.float32,\n",
    "                                           shape=[None, ACTION_SIZE],\n",
    "                                           name=\"actions\")\n",
    "            \n",
    "            # Target QVal = Reward (state | actions) + discount * QVal (new_state | total_possible_actions)\n",
    "            self.target_q = tf.placeholder(dtype=tf.float32,\n",
    "                                           shape=[None],\n",
    "                                           name=\"target_q\")\n",
    "            \n",
    "            # First Convolution\n",
    "            self.conv1 = tf.layers.conv2d(inputs=self.inputs_,\n",
    "                                          filters=32,\n",
    "                                          kernel_size=[8,8],\n",
    "                                          strides=[4,4],\n",
    "                                          padding=\"VALID\",\n",
    "                                          kernel_initializer=tf.contrib.layers.xavier_initializer_conv2d(),\n",
    "                                          name = \"conv1\")\n",
    "            \n",
    "            # Output size = (Wâˆ’K+2P)/S+1\n",
    "            # height = (108 - 8 + 2*0) / 4 + 1= 26\n",
    "            # width = (84 - 8) / 4 + 1= 20\n",
    "            \n",
    "            self.conv1_shape = self.conv1.get_shape()\n",
    "            \n",
    "            self.conv1_out = tf.nn.elu(self.conv1, name=\"conv1_out\") # shape = (batch_size, 26 ht, 20 wd, 32 filters)\n",
    "            \n",
    "            # Second Convolution\n",
    "            self.conv2 = tf.layers.conv2d(inputs=self.conv1_out,\n",
    "                                          filters=64,\n",
    "                                          kernel_size=[4,4],\n",
    "                                          strides=[2,2],\n",
    "                                          padding=\"VALID\",\n",
    "                                          kernel_initializer=tf.contrib.layers.xavier_initializer_conv2d(),\n",
    "                                          name = \"conv2\")\n",
    "            \n",
    "            self.conv2_shape = self.conv2.get_shape()\n",
    "            \n",
    "            self.conv2_out = tf.nn.elu(self.conv2, name=\"conv2_out\") # shape = (batch_size, 12 ht, 9 wd, 64 filters)\n",
    "            \n",
    "            # Third Convolution\n",
    "            self.conv3 = tf.layers.conv2d(inputs=self.conv2_out,\n",
    "                                          filters=64,\n",
    "                                          kernel_size=[3,3],\n",
    "                                          strides=[2,2],\n",
    "                                          padding=\"VALID\",\n",
    "                                          kernel_initializer=tf.contrib.layers.xavier_initializer_conv2d(),\n",
    "                                          name = \"conv3\")\n",
    "            \n",
    "            self.conv3_shape = self.conv2.get_shape()\n",
    "            \n",
    "            self.conv3_out = tf.nn.elu(self.conv3, name=\"conv3_out\") # shape = (batch size, 12 ht, 9 wd, 64 filters)\n",
    "            \n",
    "            # Flat\n",
    "            self.flat = tf.layers.flatten(self.conv3_out)\n",
    "            \n",
    "            self.flat_shape = self.flat.get_shape()\n",
    "            \n",
    "            # Hidden Layer\n",
    "            self.fully_connected = tf.layers.dense(inputs=self.flat,\n",
    "                                                   units=HIDDEN_UNITS,\n",
    "                                                   activation=tf.nn.relu,\n",
    "                                                   kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                                   name=\"fc1\")\n",
    "            \n",
    "            self.fully_connected_shape = self.fully_connected.get_shape()\n",
    "            \n",
    "            # Linear\n",
    "            self.output = tf.layers.dense(inputs=self.fully_connected,\n",
    "                                          units=ACTION_SIZE)\n",
    "            \n",
    "            self.output_shape = self.output.get_shape()\n",
    "            \n",
    "            # q val\n",
    "            self.q = tf.reduce_sum(tf.multiply(self.output, self.actions_))\n",
    "        \n",
    "            # loss = (discounted_q_vals_in_future - q_val_of_move)^2\n",
    "            self.loss = tf.reduce_mean(tf.square(self.target_q - self.q))\n",
    "            \n",
    "            # backprop\n",
    "            self.optimizer = tf.train.AdamOptimizer(LEARNING_RATE).minimize(self.loss)                    \n",
    "            \n",
    "    def print_shapes(self):\n",
    "        print(\"inputs \", self.inputs_shape)\n",
    "        print(\"c1 \", self.conv1_shape)\n",
    "        print(\"c2 \", self.conv2_shape)\n",
    "        print(\"c3 \", self.conv3_shape)\n",
    "        print(\"flat \", self.fully_connected_shape)\n",
    "        print(\"fully_connected \", self.fully_connected_shape)\n",
    "        print(\"output \", self.output_shape)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From <ipython-input-7-f5d06c3c6c72>:30: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.conv2d instead.\n",
      "WARNING:tensorflow:From /Users/katz/Applications/anaconda3/envs/rl/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From <ipython-input-7-f5d06c3c6c72>:67: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.flatten instead.\n",
      "WARNING:tensorflow:From <ipython-input-7-f5d06c3c6c72>:76: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dense instead.\n",
      "inputs  (?, 110, 84, 4)\n",
      "c1  (?, 26, 20, 32)\n",
      "c2  (?, 12, 9, 64)\n",
      "c3  (?, 12, 9, 64)\n",
      "flat  (?, 512)\n",
      "fully_connected  (?, 512)\n",
      "output  (?, 6)\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "DQN = DQN()\n",
    "\n",
    "DQN.print_shapes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Memory module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Memory():\n",
    "    def __init__(self):\n",
    "        self.buffer = deque(maxlen = MEMORY_SIZE)\n",
    "        \n",
    "    def add(self, experience):\n",
    "        self.buffer.append(experience)\n",
    "        \n",
    "    def sample(self):\n",
    "        buffer_size = len(self.buffer)\n",
    "        index = np.random.choice(np.arange(buffer_size),\n",
    "                                 size=BATCH_SIZE,\n",
    "                                 replace=False)\n",
    "        \n",
    "        return [self.buffer[i] for i in index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiences Loaded:  64\n"
     ]
    }
   ],
   "source": [
    "memory = Memory()\n",
    "\n",
    "state = env.reset() # reset game\n",
    "stack = deque([np.zeros((H,W), dtype=np.int) for i in range(STACK_SIZE)], maxlen=STACK_SIZE) # load deque with zeros\n",
    "\n",
    "# load first state, create first stack\n",
    "stack, state = stack_frames(stack, state, is_new_game=True)\n",
    "\n",
    "for i in range(LOAD_MEM):\n",
    "    \n",
    "    choice = r.randint(1, len(moves)) - 1 # random index\n",
    "    action = moves[choice] # chosen index\n",
    "    next_state, reward, done, _logs_ = env.step(choice) # act\n",
    "    \n",
    "    env.render()\n",
    "    \n",
    "    stack, next_state = stack_frames(stack, next_state, is_new_game=False) # save next state\n",
    "    \n",
    "    if done: # when we lose, create a 0-matrix for next_state\n",
    "        next_state = np.zeros(state.shape)\n",
    "    \n",
    "    memory.add((state, action, reward, next_state, done)) # add experience to memory \n",
    "    \n",
    "    state = next_state\n",
    "    \n",
    "    if done: # start new game\n",
    "        state = env.reset()\n",
    "        stack, state = stack_frames(stack, state, is_new_game=True) \n",
    "\n",
    "print(\"Experiences Loaded: \", len(memory.buffer))\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = tf.summary.FileWriter(\"./tensorboard/dqn/1\") # write\n",
    "\n",
    "tf.summary.scalar(\"Loss\", DQN.loss) \n",
    "\n",
    "write_op = tf.summary.merge_all() # merge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def move(state, moves, training_step):\n",
    "    \n",
    "    adventure = r.uniform(0, 1)\n",
    "\n",
    "    explore_prob = EXP_STOP - (EXP_START - EXP_STOP) * np.exp(-DECAY * training_step)\n",
    "\n",
    "    if explore_prob > adventure: # explore randomly\n",
    "        choice = r.randint(1, len(moves)) - 1\n",
    "        action = moves[choice]\n",
    "        \n",
    "    else: # use dqn\n",
    "        dq_table = sesh.run(DQN.output, feed_dict={DQN.inputs_:state.reshape((1, *state.shape))}) # run graph with state as input\n",
    "        choice = np.argmax(dq_table) # pick move with highest predicted q val\n",
    "        action = moves[choice]\n",
    "        \n",
    "    return action, explore_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn(DQN, memory, sesh):\n",
    "\n",
    "    batch = memory.sample()\n",
    "\n",
    "    # retrieve minibatches from memory\n",
    "    states_mini = np.array([states[0] for states in batch], ndmin=3)\n",
    "    actions_mini = np.array([actions[1] for actions in batch])\n",
    "    rewards_mini = np.array([rewards[2] for rewards in batch])\n",
    "    next_states_mini = np.array([next_states[3] for next_states in batch], ndmin=3)\n",
    "    dones_mini = np.array([dones[4] for dones in batch])\n",
    "    \n",
    "    target_qs = []\n",
    "    \n",
    "    # predict q vals for next state\n",
    "    qvals_next_state = sesh.run(DQN.output, feed_dict={DQN.inputs_:next_states_mini})\n",
    "    \n",
    "    for i in range(0, len(batch)): # loop through batches\n",
    "        \n",
    "        gameover = dones_mini[i]\n",
    "        \n",
    "        if gameover: # target_q = reward\n",
    "            target_qs.append(rewards_mini[i])\n",
    "        \n",
    "        else: # target_q = reward + gamma * max_q(s', a')\n",
    "            target = rewards_mini[i] + GAMMA * np.max(qvals_next_state[i])\n",
    "            target_qs.append(target)\n",
    "            \n",
    "    targets_mini = np.array([target for target in target_qs]) # create minibatch of target q vals\n",
    "    \n",
    "    # calculate loss and backprop on minibatches\n",
    "    loss, _ = sesh.run([DQN.loss, DQN.optimizer], feed_dict={DQN.inputs_:states_mini,\n",
    "                                                             DQN.target_q:targets_mini,\n",
    "                                                             DQN.actions_:actions_mini})\n",
    "    \n",
    "    # wrte to tensorboard during each minibatch\n",
    "    summary = sesh.run(write_op, feed_dict={DQN.inputs_:states_mini,\n",
    "                                            DQN.target_q:targets_mini,\n",
    "                                            DQN.actions_:actions_mini})\n",
    "    writer.add_summary(summary, game)\n",
    "    writer.flush()\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Game: 0 Score: 115.0 Explore Prob: -0.974 Training Loss 0.0\n",
      "Checkpoint\n",
      "Game: 1 Score: 185.0 Explore Prob: -0.966 Training Loss 9.999999747378752e-05\n",
      "Game: 2 Score: 235.0 Explore Prob: -0.955 Training Loss 0.01889999955892563\n",
      "Game: 3 Score: 230.0 Explore Prob: -0.947 Training Loss 0.007699999958276749\n",
      "Game: 4 Score: 245.0 Explore Prob: -0.941 Training Loss 0.05609999969601631\n",
      "Game: 5 Score: 500.0 Explore Prob: -0.93 Training Loss 0.12129999697208405\n",
      "Checkpoint\n",
      "Game: 6 Score: 55.0 Explore Prob: -0.926 Training Loss 0.055399999022483826\n",
      "Game: 7 Score: 235.0 Explore Prob: -0.915 Training Loss 0.08510000258684158\n",
      "Game: 8 Score: 365.0 Explore Prob: -0.906 Training Loss 0.011099999770522118\n",
      "Game: 9 Score: 225.0 Explore Prob: -0.895 Training Loss 0.020400000736117363\n",
      "Game: 10 Score: 425.0 Explore Prob: -0.885 Training Loss 0.0035000001080334187\n",
      "Checkpoint\n",
      "Game: 11 Score: 200.0 Explore Prob: -0.878 Training Loss 0.045099999755620956\n",
      "Game: 12 Score: 145.0 Explore Prob: -0.874 Training Loss 0.8312000036239624\n",
      "Game: 13 Score: 240.0 Explore Prob: -0.862 Training Loss 0.16060000658035278\n",
      "Game: 14 Score: 175.0 Explore Prob: -0.856 Training Loss 0.1720999926328659\n",
      "Game: 15 Score: 220.0 Explore Prob: -0.849 Training Loss 0.05079999938607216\n",
      "Checkpoint\n",
      "Game: 16 Score: 525.0 Explore Prob: -0.836 Training Loss 0.08470000326633453\n",
      "Game: 17 Score: 295.0 Explore Prob: -0.826 Training Loss 0.04650000110268593\n",
      "Game: 18 Score: 220.0 Explore Prob: -0.817 Training Loss 0.0\n",
      "Game: 19 Score: 435.0 Explore Prob: -0.808 Training Loss 0.006899999920278788\n",
      "Game: 20 Score: 100.0 Explore Prob: -0.803 Training Loss 0.05339999869465828\n",
      "Checkpoint\n",
      "Game: 21 Score: 520.0 Explore Prob: -0.793 Training Loss 0.0010000000474974513\n",
      "Game: 22 Score: 285.0 Explore Prob: -0.786 Training Loss 0.0\n",
      "Game: 23 Score: 300.0 Explore Prob: -0.778 Training Loss 0.0\n",
      "Game: 24 Score: 345.0 Explore Prob: -0.771 Training Loss 0.0006000000284984708\n",
      "Game: 25 Score: 260.0 Explore Prob: -0.761 Training Loss 0.0\n",
      "Checkpoint\n",
      "Game: 26 Score: 220.0 Explore Prob: -0.755 Training Loss 9.999999747378752e-05\n",
      "Game: 27 Score: 525.0 Explore Prob: -0.746 Training Loss 0.0210999995470047\n",
      "Game: 28 Score: 505.0 Explore Prob: -0.735 Training Loss 0.0020000000949949026\n",
      "Game: 29 Score: 265.0 Explore Prob: -0.729 Training Loss 0.3287000060081482\n",
      "Game: 30 Score: 235.0 Explore Prob: -0.72 Training Loss 0.0032999999821186066\n",
      "Checkpoint\n",
      "Game: 31 Score: 130.0 Explore Prob: -0.715 Training Loss 0.0835999995470047\n",
      "Game: 32 Score: 120.0 Explore Prob: -0.711 Training Loss 0.14020000398159027\n",
      "Game: 33 Score: 365.0 Explore Prob: -0.705 Training Loss 0.02280000038444996\n",
      "Game: 34 Score: 250.0 Explore Prob: -0.699 Training Loss 0.08739999681711197\n",
      "Game: 35 Score: 515.0 Explore Prob: -0.69 Training Loss 0.03200000151991844\n",
      "Checkpoint\n",
      "Game: 36 Score: 160.0 Explore Prob: -0.686 Training Loss 0.06870000064373016\n",
      "Game: 37 Score: 465.0 Explore Prob: -0.678 Training Loss 0.15520000457763672\n",
      "Game: 38 Score: 250.0 Explore Prob: -0.672 Training Loss 0.07880000025033951\n",
      "Game: 39 Score: 80.0 Explore Prob: -0.667 Training Loss 0.4893999993801117\n",
      "Game: 40 Score: 155.0 Explore Prob: -0.663 Training Loss 0.0012000000569969416\n",
      "Checkpoint\n",
      "Game: 41 Score: 210.0 Explore Prob: -0.656 Training Loss 0.3084999918937683\n",
      "Game: 42 Score: 235.0 Explore Prob: -0.648 Training Loss 0.0013000000035390258\n",
      "Game: 43 Score: 230.0 Explore Prob: -0.643 Training Loss 0.29679998755455017\n",
      "Game: 44 Score: 130.0 Explore Prob: -0.639 Training Loss 0.08590000122785568\n",
      "Game: 45 Score: 185.0 Explore Prob: -0.634 Training Loss 0.032999999821186066\n",
      "Checkpoint\n",
      "Game: 46 Score: 65.0 Explore Prob: -0.632 Training Loss 0.0005000000237487257\n",
      "Game: 47 Score: 180.0 Explore Prob: -0.628 Training Loss 0.0003000000142492354\n",
      "Game: 48 Score: 145.0 Explore Prob: -0.624 Training Loss 0.10599999874830246\n",
      "Game: 49 Score: 235.0 Explore Prob: -0.616 Training Loss 0.21660000085830688\n",
      "Game: 50 Score: 165.0 Explore Prob: -0.611 Training Loss 0.3555999994277954\n",
      "Checkpoint\n",
      "Game: 51 Score: 205.0 Explore Prob: -0.607 Training Loss 0.13940000534057617\n",
      "Game: 52 Score: 440.0 Explore Prob: -0.6 Training Loss 0.0013000000035390258\n",
      "Game: 53 Score: 185.0 Explore Prob: -0.594 Training Loss 0.17329999804496765\n",
      "Game: 54 Score: 190.0 Explore Prob: -0.59 Training Loss 0.08399999886751175\n",
      "Game: 55 Score: 375.0 Explore Prob: -0.584 Training Loss 24.294599533081055\n",
      "Checkpoint\n",
      "Game: 56 Score: 480.0 Explore Prob: -0.579 Training Loss 0.0\n"
     ]
    }
   ],
   "source": [
    "saver = tf.train.Saver() \n",
    "\n",
    "if TRAINING == True:\n",
    "    \n",
    "    with tf.Session() as sesh:\n",
    "        \n",
    "        tf.global_variables_initializer().run()\n",
    "        \n",
    "        training_step = 0\n",
    "        \n",
    "        for game in range(NUM_GAMES+1): # play games\n",
    "            \n",
    "            # initialize\n",
    "            game_step = 0\n",
    "            game_rewards = []\n",
    "            state = env.reset()\n",
    "            \n",
    "            stack, state = stack_frames(stack, state, is_new_game=True)\n",
    "            \n",
    "            while game_step < MAX_STEPS:\n",
    "                \n",
    "                game_step += 1\n",
    "                training_step += 1\n",
    "                \n",
    "                action, explore_prob = move(state, moves, training_step) # choose move\n",
    "                \n",
    "                next_state, reward, done, _logs_ = env.step(np.argmax(action)) # act\n",
    "                \n",
    "                if ENV_RENDER: # show games\n",
    "                    env.render()\n",
    "                    \n",
    "                game_rewards.append(reward) # track reward\n",
    "                \n",
    "                if done: # we've lost\n",
    "                    next_state = np.zeros(state.shape)\n",
    "                    stack, next_state = stack_frames(stack, next_state, is_new_game=False) \n",
    "                    game_step = MAX_STEPS\n",
    "                    total_reward = np.sum(game_rewards)\n",
    "                    memory.add((state, action, reward, next_state, done))\n",
    "                    \n",
    "                    print('Game: {}'.format(game), \n",
    "                          'Score: {}'.format(round(total_reward, 3)),\n",
    "                          'Explore Prob: {}'.format(round(explore_prob, 3)),\n",
    "                          'Training Loss {}'.format(round(loss, 4)))\n",
    "        \n",
    "                else: # we're still alive!\n",
    "                    stack, next_state = stack_frames(stack, next_state, is_new_game=False)\n",
    "                    memory.add((state, action, reward, next_state, done))\n",
    "                    state = next_state\n",
    "                \n",
    "                loss = learn(DQN, memory, sesh)\n",
    "                \n",
    "            if game % 5 == 0: # save model every five games\n",
    "                save_path = saver.save(sesh, './models/model.cpkt')\n",
    "                print('Checkpoint')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%bash\n",
    "# tensorboard --logdir='./tensorboard/dqn'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sesh:\n",
    "    total_test_rewards = []\n",
    "    \n",
    "    saver.restore(sesh, \"./models/model.cpkt\")\n",
    "    \n",
    "    for game in range(3):\n",
    "        total_rewards = 0\n",
    "        state = env.reset()\n",
    "        stack, state = stack_frames(stack, state, is_new_game=True)\n",
    "        \n",
    "        print(\"****************************************************\")\n",
    "        print(\"GAME \", game)\n",
    "        \n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            state = state.reshape((1, *STATE_SIZE))\n",
    "            q_vals = sesh.run(DQN.output, feed_dict={DQN.inputs_:state})\n",
    "            \n",
    "            next_state, reward, done, _logs_ = env.step(np.argmax(q_vals))\n",
    "            \n",
    "            env.render()\n",
    "            \n",
    "            total_rewards += reward\n",
    "            \n",
    "            if done:\n",
    "                print(\"Score: \", total_rewards)\n",
    "                total_test_rewards.append(total_rewards)\n",
    "            \n",
    "            stack, next_state = stack_frames(stack, next_state, is_new_game=False)\n",
    "            state = next_state\n",
    "            \n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
