{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import tensorflow as tf\n",
    "import retro\n",
    "\n",
    "import numpy as np\n",
    "import random as r\n",
    "\n",
    "from skimage import transform # to preprocess game frames\n",
    "from skimage.color import rgb2gray # to convert game frames into grayscale\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import imshow\n",
    "from PIL import Image\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import models, layers, datasets\n",
    "\n",
    "from collections import deque # to create ordered collections of frames\n",
    "\n",
    "import warnings # ignore warning messages from skiimage during training\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frame Size:  Box(210, 160, 3)\n",
      "Action Space:  6  moves\n",
      "\n",
      "This is what the agent sees: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1326da470>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAANQAAAD8CAYAAAAPIYpDAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAD/RJREFUeJzt3W2sHOV5xvH/VTvQkjTCrmPqwmntVCapE7UJItRR1Ig0SuI2EUaqqIxEY7WOrFZJlVatiO18oF84skjVlw9NJStFcdQU4pIQUJMCDi2llQLhQHDBgIMLFA64uAnpWxqB3Nz9MHPCstk9OzvzzM7MzvWTVmd3dvbZe72+zzMzO3sdRQRmlsYPNV2A2TxxQ5kl5IYyS8gNZZaQG8osITeUWUK1NZSkHZJOSDopaV9dz2PWJqrjcyhJa4BvAO8BloH7gCsj4pHkT2bWInXNUJcAJyPiiYh4CbgR2FnTc5m1xtqaxj0feGbg9jLw8+NWluTTNaztvhkRr5u0Ul0NpRHLXtE0kvYCe2t6frPU/rXISnU11DKwMHD7AuC5wRUi4hBwCDxD2fyoax/qPmCrpC2SzgJ2AbfW9FxmrVHLDBURZyR9BLgdWANcHxHH63guszap5bD51EV4k8/a7/6IuHjSSnN3psTi4sLklRoe0zXOr7lrKLNGRUTjF7JD6oUvi4sLUy2vMl6ZMVd7nGvs7GWp0P/lpptpmoYafGOG36Cq/wmG3/QU/6lcY/nxWngp1FB1fQ5VuwMHnnnFNvmBA8+ssnax8eDl7fyq462M4Rqr19gl3ocyS6izh83HHTEq+xtxtSNQqcd0jZ00v4fNR21ODG9qlBlvcJzBsacdc3FxwTUmqLGLOjlDpf4tOOmNnnbM1OPVMWYXamyZ+Z2hzNrKDWWWUKcPmw+rso2eerw6xuxrjV3SqX2o4Tdm3A7vNNvq48Zc7blcY/01tlChfahONZRZg3xQwmzW3FBmCbmhzBIq3VCSFiT9vaRHJR2X9NF8+XpJRyU9nv9cl65cs3arMkOdAX4vIn4G2A58WNI2YB9wZ0RsBe7Mb5v1QumGiohTEfFAfv2/gUfJAi53Aofz1Q4Dl1ct0qwrkuxDSdoMvBW4FzgvIk5B1nTAxhTPYdYFlc+UkPQa4PPA70TEf0mjQmNHPs7JsTZ3Ks1Qkl5F1kyfjYgv5Iufl7Qpv38TcHrUYyPiUERcXOTDMrOuKD1DKZuK/gJ4NCL+aOCuW4HdwMH85y2VKhxj1PlhVU5rST1eHWP2tcYuqbLJ9w7g14CHJD2YLztA1khHJO0BngauqFaiWXd08lw+f8Gw+phdqLFl5vtcvuE3J0Vaz6ivglcdc7XbZcbrY41d0qkZquj3aqp87aDqmK4x3ZgtM98zlFkrNZ0amzo5dtqk0iKJp9OMWSSV1TV28uLk2GnHg/6lsnahxi7xJp9ZQp2doRYXF34gCyHFB5KDv2FTfGjqGqvX2CWeocwSckOZJeSGMkuoUx/smjXIH+yazZobyiwhN5RZQm4os4TcUGYJuaHMEqrcUJLWSPq6pL/Jbzs51norxQz1UbKQyxVOjrXeqhojdgHwfuBTA4udHGu9VXWG+hPgauB7A8sKJcdK2itpSdJSxRrMWqPKX9/4AHA6Iu4v8/gyQZeLiwsTswum/XuuRdafZszU49UxZhdq7KqquXyXSfpl4IeB10r6S/Lk2Ig4tVpyrNk8qvLXN/ZHxAURsRnYBfxdRFzFy8mxMMPk2Kq//YZnvxS/TV1jP2alQXV8Y3dmybF1vFmpx3SN/ZKkoSLiLuCu/Pq3gHenGNesc5qOEJsmRmzlMhxZVTb6ani8wTGHr087nmusXmPLLvMbIzYu9KNsGMisxqtjzHmusYt8Lp9ZQm4os4TcUGYpNX1AosxBCUbsOFfd6V0tk7vKmK6x8wcjVi6FDkp0KvVoUgZ3mTzt1R5TJvPbNaapsYUKpR51qqHMGuQYMbNZc0OZJeSGMkvIDWWWkBvKLCE3lFlCbiizhNxQZglVjRE7V9JNkh6T9Kiktzvo0vqs6gz1p8BtEfFG4OfIAi8ddGm9VfrUI0mvBY4Br4+BQSSdAC4dSD26KyLeMGGsqYsYlVmQ4q+XpxqvjjH7WmNL1Hsun6S3AIeAR8hmp/vJYpmfjYhzB9b7dkSsutk3bUOtFgBS5s2bFCgy7Zipx6tjzC7U2DK1n8u3FrgI+POIeCvwHabYvCuTHDscUTX4Bq1crxrQuDLO4NhVQiRdY7kau6pKQy0DyxFxb377JrIGez7f1GO1oMsyybFmbVfp6xuS/hH4UESckPQHwKvzu74VEQcl7QPWR8TVE8bxJt+Mx+xCjS1T//eh8v2oTwFnAU8Av0426x0BfpI86DIiXpgwjg9KNDBeHWP2/aBEpRixiHgQGPUkMwm6rGP7vOw+xKTx6hizTzV2hc+UMEvIDWWWUtOJR1WimIsurzJe1Zhj19ivKGbPUGYpNT07TTNDFflNN+1vwiLrTzNm6vH6WmMLL/OXy2fWIMeImc2aG8osITeUWUJuKLOE3FBmCbmhzBJyQ5kl5IYyS8gNZZaQG8osoUpfMGxSF75p6hqrj9c1Vb8C/7vAh8hOHnyI7Cvw5wCfAzYDTwG/GhHfnjDOVEU4U6L6mF2osWVqz+U7H/gnYFtEfFfSEeDLwDbghYGQlnUR8bEJYxUqYtJvvyp/bHnUmFX/ILRrLF9jC83k5Ni1wI9IWks2Mz0H7AQO5/cfBi6v+BxmnVG6oSLiWeAPyZKNTgH/GRF3AOdFxKl8nVPAxlGPLxN0adZ2pQ9K5H9VYyewBfgP4K8lXVX08RFxiCzKudL3oepI1Uk9pmvsjyr7UFcAOyJiT377g8B2sgixWv9YwOLiAgcOPLNqpHCZ8Vaupxhv5fGusdP7TYNqz+V7Gtgu6Rzgu2SNtESWcb4bOJj/vKXCc6yqC3lyrrFfs1TphoqIeyXdBDwAnAG+TrYJ9xrgiKQ95MmxKQo164KqybHXANcMLX6RGSXHmrWNTz0yS6jzpx5V/UBy+LGDj6+6Y+0a5/YAxVieocwSci6fWTHO5TObNTeUWUJuKLOE3FBmCbmhzBJyQ5kl5IYyS8gNZZaQG8osoc6eywf1nCNW5Ty21carY8w+1dgVnZyhxn1preyX2VYbr8yYqz3ONc63TjVUkTdm2jeuyPrTjJl6vDrG7EKNXTWxoSRdL+m0pIcHlq2XdFTS4/nPdQP37Zd0UtIJSe+rq3CzNpp4trmkdwL/A3wmIt6cL7uOEWGWkrYBNwCXAD8BfAW4MCL+b8JzlA66HKVqQGOVMV1jujFbptDZ5kTExAtZrPLDA7dPAJvy65uAE/n1/cD+gfVuB95eYPyY5rK4uDDV8irjlRlztce5xs5elor0Stl9qHFhlucDg7+ClvNlZr2Q+rC5RiyLkStKe4G9iZ/frFFlG+p5SZsGwixP58uXgcGN6QvI8s5/QKrk2BWDIYsppB6vjjH7WmOrldyH+gSwL7++D7guv/4m4BhwNllE8xPAmlT7UKO2xatun496bB3jucbOXwrtQ02coSTdAFwKbJC0TJbDd5ARYZYRcTz/szaPkIVffnjSET6zeTKxoSLiyjF3jQyzjIhrgWurFFVUXzd5+lhjV3TqTAmztnOMmFkxjhEzmzU3lFlCbiizhNxQZgm5ocwSckOZJeSGMkvIDWWWkBvKLCE3lFlCbiizhNxQZgm5ocwSckOZJeSGMkuobHLsJyQ9JumfJd0s6dyB+5wca71VZIb6NLBjaNlR4M0R8bPAN8gCLsmTY3eRhbXsAD4paU2yas1abmJDRcTdwAtDy+6IiDP5zXvI4sIAdgI3RsSLEfEkcJIsltmsF1LsQ/0G8Lf59cLJsZL2SlqStJSgBrNWqJQcK+njZHFhn11ZNGK1kXkRqYMuzdqgdENJ2g18AHh3vJz0Ujg51mweldrkk7QD+BhwWUT878BdtwK7JJ0taQuwFfha9TLNuqFscux+srjlo5IA7omI33RyrPWdc/nMinEun9msuaHMEnJDmSXkhjJLyA1llpAbyiwhN5RZQm4os4QqnRw77770K28cufz9n39sxpWMN6rGNtUH3agxFc9QZgm5ocwS8rl8Y4zb3FvRhk0W1zhTPpfPbNbcUGYJuaHMEnJDmSVUKuhy4L7flxSSNgwsc9Cl9VbZoEskLQDvAZ4eWOagS+u1UkGXuT8GruaVMWEOurReK5t6dBnwbEQcG7qrcNCl2Tya+lw+SecAHwfeO+ruEctGfmgraS+wd9rnN2uzMjPUTwNbgGOSniILs3xA0o8zRdBlRByKiIuLfPo8a5M+3S+6Tp1cYztN3VAR8VBEbIyIzRGxmayJLoqIf8NBl9ZzRQ6b3wB8FXiDpGVJe8atGxHHgZWgy9tw0KX1zMR9qIi4csL9m4duXwtcW60ss27ymRJmCbmhzBJyQ5kl5IYyS8ghLasY923SNn12MqrGNtUH3agxFc9QZgk5U8KsGGdKmM2aG8osITeUWUJuKLOE3FBmCbmhzBJyQ5kl5IYyS8gNZZaQG8osodLJsZJ+O0+HPS7puoHlTo61/oqIVS/AO4GLgIcHlr0L+Apwdn57Y/5zG3AMOJssGelfgDUFniN88aXll6VJ/48jolCmxN2SNg8t/i3gYES8mK9zOl/+/eRY4ElJK8mxX530PFa/zyxuL/3YDx64J2El86vsPtSFwC9IulfSP0h6W77cybHWa2W/YLgWWAdsB94GHJH0epwc22pFZ5kqM1nflW2oZeALke0AfU3S94ANTJkcCxwCfx/K5kfZTb4vAr8IIOlC4Czgmzg51npu4gyVJ8deCmyQtAxcA1wPXJ8fSn8J2J3PVsclrSTHnsHJsa3iTbn6VUmOvWrM+k6Otd7ymRJmCTlGrEd8lK9+nqHMEvIM1SOeeernGcosITeUWUKtSI5d92Ovikvft77pMszG+uINpwslx7aioST9O/AdsrMt+moDfv1tfv0/FRGvm7RSKxoKQNJSG/8i/Kz49c/H6/c+lFlCbiizhNrUUIeaLqBhfv1zoDX7UGbzoE0zlFnnNd5QknbkkWMnJe1rup5ZkPSUpIckPShpKV+2XtJRSY/nP9c1XWcqo6LoVnu9XY6ia7ShJK0B/gz4JbIIsislbWuyphl6V0S8ZeBQ8T7gzojYCtyZ354XnwZ2DC0b+Xrz938X8Kb8MZ/M/590QtMz1CXAyYh4IiJeAm4kiyLro53A4fz6YeDyBmtJKiLuBl4YWjzu9X4/ii4ingRWoug6oemG6mvsWAB3SLo/T38COC8iTgHkPzc2Vt1sjHu9nf4/0fTXNwrHjs2Zd0TEc5I2AkclPdZ0QS3S6f8TTc9QhWPH5klEPJf/PA3cTLZJ87ykTQD5z9PjR5gL415vp/9PNN1Q9wFbJW2RdBbZzuitDddUK0mvlvSjK9eB9wIPk73u3flqu4FbmqlwZsa93k5H0TW6yRcRZyR9BLgdWANcHxHHm6xpBs4DbpYE2b//X0XEbZLuI0vg3QM8DVzRYI1JjYmiO8iI1xsRnY6i85kSZgk1vclnNlfcUGYJuaHMEnJDmSXkhjJLyA1llpAbyiwhN5RZQv8PRRCvxCY7TyEAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "env = gym.make('SpaceInvaders-v0') # make env\n",
    "\n",
    "print(\"Frame Size: \", env.observation_space)\n",
    "print(\"Action Space: \", env.action_space.n, \" moves\")\n",
    "\n",
    "moves = np.array(np.identity(env.action_space.n, dtype=int).tolist())\n",
    "\n",
    "UP = 26\n",
    "DOWN = -14\n",
    "LEFT = 14\n",
    "RIGHT = -14\n",
    "\n",
    "H = 210 - (UP - DOWN) # height of preprocessed image\n",
    "W = 160 - (LEFT - RIGHT) # width of preprocessed image\n",
    "\n",
    "obs = env.reset()[UP:DOWN, LEFT:RIGHT]\n",
    "\n",
    "%matplotlib inline\n",
    "img = Image.fromarray(obs, 'RGB')\n",
    "\n",
    "print(\"\\nThis is what the agent sees: \")\n",
    "imshow(np.asarray(img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input size (210, 160, 3)\n"
     ]
    }
   ],
   "source": [
    "for i_episode in range(1):\n",
    "    observation = env.reset()\n",
    "    print(\"input size\", observation.shape)\n",
    "#     for t in range(1):\n",
    "#         env.render()\n",
    "#         print(observation)\n",
    "#         action = env.action_space.sample()\n",
    "#         observation, reward, done, info = env.step(action)\n",
    "#         if done:\n",
    "#             print(\"Episode finished after {} timesteps\".format(t+1))\n",
    "#             break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "STACK_SIZE = 4 # \"phi length\" - how many frames we consolidate into one training example\n",
    "STATE_SIZE = [H, W, STACK_SIZE] # stack dimensions\n",
    "ACTION_SIZE = env.action_space.n # 6 possible moves\n",
    "LEARNING_RATE = 2.5e-4\n",
    "\n",
    "# training\n",
    "NUM_GAMES = 250 # how many games we will play for training\n",
    "MAX_STEPS = 50000 # max number of moves we'll make each game\n",
    "HIDDEN_UNITS = 512\n",
    "BATCH_SIZE = 1 # num of states we include for each weight update in gradient descent\n",
    "TRAINING = True # when you just want agent to play\n",
    "ENV_RENDER = True # when you want to see env\n",
    "\n",
    "# explore params\n",
    "EXP_START = 1.0\n",
    "EXP_STOP = 0.01\n",
    "DECAY = 1e-5\n",
    "\n",
    "# q le table\n",
    "GAMMA = 0.001 # discount for future rewards in bellman equation\n",
    "\n",
    "# memory\n",
    "LOAD_MEM = 64 # num experiences to be stored in memory during initialization\n",
    "MEMORY_SIZE = int(1e6) # max num experiences stored in memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess raw input from game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def state_to_frame(state):\n",
    "    \n",
    "#     print(\"Input Size: \", state.shape)\n",
    "    \n",
    "    gray_doe = rgb2gray(state)\n",
    "    crop_doe = gray_doe[UP:DOWN, LEFT:RIGHT]\n",
    "    frame = crop_doe / 255.0\n",
    "    \n",
    "#     print(\"Output Size: \", frame.shape)\n",
    "    \n",
    "    return  frame # Shape = 172 height, 132 width | vals = [0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stack_frames(stack_o_frames, state, is_new_game):\n",
    "    \n",
    "    frame = state_to_frame(state)\n",
    "    \n",
    "    if is_new_game: # if new game, clear stack from last game\n",
    "        \n",
    "        for i in range(STACK_SIZE): # in new game, start with copies of OG frame\n",
    "            stack_o_frames.append(frame)\n",
    "            \n",
    "#         print(\"Stack Size: \", len(stack_o_frames))  \n",
    "#         print(\"State Size: \", state.shape)\n",
    "        \n",
    "    else: # if we already have stack, append latest frame\n",
    "        \n",
    "        stack_o_frames.append(frame) # deque automatically removes oldest frame\n",
    "        \n",
    "    state = np.stack(stack_o_frames, axis=2) # input to neural net, shape = (108, 84, 4)\n",
    "       \n",
    "#     print(\"Snapshot: \", state[:5])\n",
    "        \n",
    "    return stack_o_frames, state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_12 (Conv2D)           (None, 41, 32, 32)        8224      \n",
      "_________________________________________________________________\n",
      "conv2d_13 (Conv2D)           (None, 19, 15, 64)        32832     \n",
      "_________________________________________________________________\n",
      "conv2d_14 (Conv2D)           (None, 9, 7, 64)          36928     \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 4032)              0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 64)                258112    \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 6)                 390       \n",
      "=================================================================\n",
      "Total params: 336,486\n",
      "Trainable params: 336,486\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "class dqn:\n",
    "    def __init__(self, name='dqn'):\n",
    "        \n",
    "        self.model = models.Sequential()\n",
    "        \n",
    "        self.model.add(layers.Conv2D(filters=32, \n",
    "                                kernel_size=(8, 8), \n",
    "                                strides=4,\n",
    "                                activation='relu', \n",
    "                                input_shape=(STATE_SIZE)))\n",
    "        \n",
    "        self.model.add(layers.Conv2D(filters=64, \n",
    "                                kernel_size=(4, 4),\n",
    "                                strides=2,\n",
    "                                activation='relu'))\n",
    "        \n",
    "        self.model.add(layers.Conv2D(filters=64, \n",
    "                                kernel_size=(3, 3),\n",
    "                                strides=2,\n",
    "                                activation='relu'))\n",
    "        \n",
    "        self.model.add(layers.Flatten())\n",
    "        \n",
    "        self.model.add(layers.Dense(64, activation='relu'))\n",
    "        \n",
    "        self.model.add(layers.Dense(ACTION_SIZE, activation='softmax'))\n",
    "        \n",
    "        model.compile(optimizer='adam',\n",
    "              loss='mse',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "hello = dqn(name='yo')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Memory module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Memory():\n",
    "    def __init__(self):\n",
    "        self.buffer = deque(maxlen = MEMORY_SIZE)\n",
    "        \n",
    "    def add(self, experience):\n",
    "        self.buffer.append(experience)\n",
    "        \n",
    "    def sample(self):\n",
    "        buffer_size = len(self.buffer)\n",
    "        index = np.random.choice(np.arange(buffer_size),\n",
    "                                 size=BATCH_SIZE,\n",
    "                                 replace=False)\n",
    "        \n",
    "        return [self.buffer[i] for i in index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiences Loaded:  64\n"
     ]
    }
   ],
   "source": [
    "memory = Memory()\n",
    "\n",
    "state = env.reset() # reset game\n",
    "stack = deque([np.zeros((H,W), dtype=np.int) for i in range(STACK_SIZE)], maxlen=STACK_SIZE) # load deque with zeros\n",
    "\n",
    "# load first state, create first stack\n",
    "stack, state = stack_frames(stack, state, is_new_game=True)\n",
    "\n",
    "for i in range(LOAD_MEM):\n",
    "    \n",
    "    choice = r.randint(1, len(moves)) - 1 # random index\n",
    "    action = moves[choice] # chosen index\n",
    "    next_state, reward, done, _logs_ = env.step(choice) # act\n",
    "    \n",
    "    env.render()\n",
    "    \n",
    "    stack, next_state = stack_frames(stack, next_state, is_new_game=False) # save next state\n",
    "    \n",
    "    if done: # when we lose, create a 0-matrix for next_state\n",
    "        next_state = np.zeros(state.shape)\n",
    "    \n",
    "    memory.add((state, action, reward, next_state, done)) # add experience to memory \n",
    "    \n",
    "    state = next_state\n",
    "    \n",
    "    if done: # start new game\n",
    "        state = env.reset()\n",
    "        stack, state = stack_frames(stack, state, is_new_game=True) \n",
    "\n",
    "print(\"Experiences Loaded: \", len(memory.buffer))\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorboard.summary._tf.summary' has no attribute 'FileWriter'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-71-8e3038c2ea3b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mwriter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFileWriter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./tensorboard/dqn/1\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# write\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Loss\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDQN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mwrite_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# merge\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'tensorboard.summary._tf.summary' has no attribute 'FileWriter'"
     ]
    }
   ],
   "source": [
    "writer = tf.summary.FileWriter(\"./tensorboard/dqn/1\") # write\n",
    "\n",
    "tf.summary.scalar(\"Loss\", DQN.loss) \n",
    "\n",
    "write_op = tf.summary.merge_all() # merge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def move(state, moves, training_step):\n",
    "    \n",
    "    adventure = r.uniform(0, 1)\n",
    "\n",
    "    explore_prob = EXP_STOP - (EXP_START - EXP_STOP) * np.exp(-DECAY * training_step)\n",
    "\n",
    "    if explore_prob > adventure: # explore randomly\n",
    "        choice = r.randint(1, len(moves)) - 1\n",
    "        action = moves[choice]\n",
    "        \n",
    "    else: # use dqn\n",
    "        dq_table = sesh.run(DQN.output, feed_dict={DQN.inputs_:state.reshape((1, *state.shape))}) # run graph with state as input\n",
    "        choice = np.argmax(dq_table) # pick move with highest predicted q val\n",
    "        action = moves[choice]\n",
    "        \n",
    "    return action, explore_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn(DQN, memory, sesh):\n",
    "\n",
    "    batch = memory.sample()\n",
    "\n",
    "    # retrieve minibatches from memory\n",
    "    states_mini = np.array([states[0] for states in batch], ndmin=3)\n",
    "    actions_mini = np.array([actions[1] for actions in batch])\n",
    "    rewards_mini = np.array([rewards[2] for rewards in batch])\n",
    "    next_states_mini = np.array([next_states[3] for next_states in batch], ndmin=3)\n",
    "    dones_mini = np.array([dones[4] for dones in batch])\n",
    "    \n",
    "    target_qs = []\n",
    "    \n",
    "    # predict q vals for next state\n",
    "    qvals_next_state = sesh.run(DQN.output, feed_dict={DQN.inputs_:next_states_mini})\n",
    "    \n",
    "    for i in range(0, len(batch)): # loop through batches\n",
    "        \n",
    "        gameover = dones_mini[i]\n",
    "        \n",
    "        if gameover: # target_q = reward\n",
    "            target_qs.append(rewards_mini[i])\n",
    "        \n",
    "        else: # target_q = reward + gamma * max_q(s', a')\n",
    "            target = rewards_mini[i] + GAMMA * np.max(qvals_next_state[i])\n",
    "            target_qs.append(target)\n",
    "            \n",
    "    targets_mini = np.array([target for target in target_qs]) # create minibatch of target q vals\n",
    "    \n",
    "    # calculate loss and backprop on minibatches\n",
    "    \n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saver = tf.train.Saver() \n",
    "\n",
    "if TRAINING == True:\n",
    "    \n",
    "    with tf.Session() as sesh:\n",
    "        \n",
    "        tf.global_variables_initializer().run()\n",
    "        \n",
    "        training_step = 0\n",
    "        \n",
    "        for game in range(NUM_GAMES+1): # play games\n",
    "            \n",
    "            # initialize\n",
    "            game_step = 0\n",
    "            game_rewards = []\n",
    "            state = env.reset()\n",
    "            \n",
    "            stack, state = stack_frames(stack, state, is_new_game=True)\n",
    "            \n",
    "            while game_step < MAX_STEPS:\n",
    "                \n",
    "                game_step += 1\n",
    "                training_step += 1\n",
    "                \n",
    "                action, explore_prob = move(state, moves, training_step) # choose move\n",
    "                \n",
    "                next_state, reward, done, _logs_ = env.step(np.argmax(action)) # act\n",
    "                \n",
    "                if ENV_RENDER: # show games\n",
    "                    env.render()\n",
    "                    \n",
    "                game_rewards.append(reward) # track reward\n",
    "                \n",
    "                if done: # we've lost\n",
    "                    next_state = np.zeros(state.shape)\n",
    "                    stack, next_state = stack_frames(stack, next_state, is_new_game=False) \n",
    "                    game_step = MAX_STEPS\n",
    "                    total_reward = np.sum(game_rewards)\n",
    "                    memory.add((state, action, reward, next_state, done))\n",
    "                    \n",
    "                    print('Game: {}'.format(game), \n",
    "                          'Score: {}'.format(round(total_reward, 3)),\n",
    "                          'Explore Prob: {}'.format(round(explore_prob, 3)),\n",
    "                          'Training Loss {}'.format(round(loss, 4)))\n",
    "        \n",
    "                else: # we're still alive!\n",
    "                    stack, next_state = stack_frames(stack, next_state, is_new_game=False)\n",
    "                    memory.add((state, action, reward, next_state, done))\n",
    "                    state = next_state\n",
    "                \n",
    "                loss = learn(DQN, memory, sesh)\n",
    "                \n",
    "            if game % 5 == 0: # save model every five games\n",
    "                save_path = saver.save(sesh, './models/model.cpkt')\n",
    "                print('Checkpoint')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%bash\n",
    "# tensorboard --logdir='./tensorboard/dqn'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sesh:\n",
    "    total_test_rewards = []\n",
    "    \n",
    "    saver.restore(sesh, \"./models/model.cpkt\")\n",
    "    \n",
    "    for game in range(3):\n",
    "        total_rewards = 0\n",
    "        state = env.reset()\n",
    "        stack, state = stack_frames(stack, state, is_new_game=True)\n",
    "        \n",
    "        print(\"****************************************************\")\n",
    "        print(\"GAME \", game)\n",
    "        \n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            state = state.reshape((1, *STATE_SIZE))\n",
    "            q_vals = sesh.run(DQN.output, feed_dict={DQN.inputs_:state})\n",
    "            \n",
    "            next_state, reward, done, _logs_ = env.step(np.argmax(q_vals))\n",
    "            \n",
    "            env.render()\n",
    "            \n",
    "            total_rewards += reward\n",
    "            \n",
    "            if done:\n",
    "                print(\"Score: \", total_rewards)\n",
    "                total_test_rewards.append(total_rewards)\n",
    "            \n",
    "            stack, next_state = stack_frames(stack, next_state, is_new_game=False)\n",
    "            state = next_state\n",
    "            \n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN:\n",
    "    def __init__(self, name=\"DQN\"):\n",
    "        \n",
    "        with tf.variable_scope(name): # create model skeleton, using placeholders for data to be fed in\n",
    "            \n",
    "            self.inputs_ = tf.placeholder(dtype=tf.float32, \n",
    "                                          shape=[None, *STATE_SIZE], \n",
    "                                          # *STATE_SIZE means we take each element of STATE_SIZE and input sequentially\n",
    "                                          # ie. [None, 110, 84, 4] instead of [None, [110, 84, 4]]\n",
    "                                          name=\"inputs\")\n",
    "            \n",
    "            self.inputs_shape = self.inputs_.get_shape()\n",
    "            \n",
    "            self.actions_ = tf.placeholder(dtype=tf.float32,\n",
    "                                           shape=[None, ACTION_SIZE],\n",
    "                                           name=\"actions\")\n",
    "            \n",
    "            # Target QVal = Reward (state | actions) + discount * QVal (new_state | total_possible_actions)\n",
    "            self.target_q = tf.placeholder(dtype=tf.float32,\n",
    "                                           shape=[None],\n",
    "                                           name=\"target_q\")\n",
    "            \n",
    "            # First Convolution\n",
    "            self.conv1 = tf.layers.conv2d(inputs=self.inputs_,\n",
    "                                          filters=32,\n",
    "                                          kernel_size=[8,8],\n",
    "                                          strides=[4,4],\n",
    "                                          padding=\"VALID\",\n",
    "                                          kernel_initializer=tf.contrib.layers.xavier_initializer_conv2d(),\n",
    "                                          name = \"conv1\")\n",
    "            \n",
    "            # Output size = (W−K+2P)/S+1\n",
    "            # height = (108 - 8 + 2*0) / 4 + 1= 26\n",
    "            # width = (84 - 8) / 4 + 1= 20\n",
    "            \n",
    "            self.conv1_shape = self.conv1.get_shape()\n",
    "            \n",
    "            self.conv1_out = tf.nn.elu(self.conv1, name=\"conv1_out\") # shape = (batch_size, 26 ht, 20 wd, 32 filters)\n",
    "            \n",
    "            # Second Convolution\n",
    "            self.conv2 = tf.layers.conv2d(inputs=self.conv1_out,\n",
    "                                          filters=64,\n",
    "                                          kernel_size=[4,4],\n",
    "                                          strides=[2,2],\n",
    "                                          padding=\"VALID\",\n",
    "                                          kernel_initializer=tf.contrib.layers.xavier_initializer_conv2d(),\n",
    "                                          name = \"conv2\")\n",
    "            \n",
    "            self.conv2_shape = self.conv2.get_shape()\n",
    "            \n",
    "            self.conv2_out = tf.nn.elu(self.conv2, name=\"conv2_out\") # shape = (batch_size, 12 ht, 9 wd, 64 filters)\n",
    "            \n",
    "            # Third Convolution\n",
    "            self.conv3 = tf.layers.conv2d(inputs=self.conv2_out,\n",
    "                                          filters=64,\n",
    "                                          kernel_size=[3,3],\n",
    "                                          strides=[2,2],\n",
    "                                          padding=\"VALID\",\n",
    "                                          kernel_initializer=tf.contrib.layers.xavier_initializer_conv2d(),\n",
    "                                          name = \"conv3\")\n",
    "            \n",
    "            self.conv3_shape = self.conv2.get_shape()\n",
    "            \n",
    "            self.conv3_out = tf.nn.elu(self.conv3, name=\"conv3_out\") # shape = (batch size, 12 ht, 9 wd, 64 filters)\n",
    "            \n",
    "            # Flat\n",
    "            self.flat = tf.layers.flatten(self.conv3_out)\n",
    "            \n",
    "            self.flat_shape = self.flat.get_shape()\n",
    "            \n",
    "            # Hidden Layer\n",
    "            self.fully_connected = tf.layers.dense(inputs=self.flat,\n",
    "                                                   units=HIDDEN_UNITS,\n",
    "                                                   activation=tf.nn.relu,\n",
    "                                                   kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                                   name=\"fc1\")\n",
    "            \n",
    "            self.fully_connected_shape = self.fully_connected.get_shape()\n",
    "            \n",
    "            # Linear\n",
    "            self.output = tf.layers.dense(inputs=self.fully_connected,\n",
    "                                          units=ACTION_SIZE)\n",
    "            \n",
    "            self.output_shape = self.output.get_shape()\n",
    "            \n",
    "            # q val\n",
    "            self.q = tf.reduce_sum(tf.multiply(self.output, self.actions_))\n",
    "        \n",
    "            # loss = (discounted_q_vals_in_future - q_val_of_move)^2\n",
    "            self.loss = tf.reduce_mean(tf.square(self.target_q - self.q))\n",
    "            \n",
    "            # backprop\n",
    "            self.optimizer = tf.train.AdamOptimizer(LEARNING_RATE).minimize(self.loss)                    \n",
    "            \n",
    "    def print_shapes(self):\n",
    "        print(\"inputs \", self.inputs_shape)\n",
    "        print(\"c1 \", self.conv1_shape)\n",
    "        print(\"c2 \", self.conv2_shape)\n",
    "        print(\"c3 \", self.conv3_shape)\n",
    "        print(\"flat \", self.fully_connected_shape)\n",
    "        print(\"fully_connected \", self.fully_connected_shape)\n",
    "        print(\"output \", self.output_shape)\n",
    "            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
