{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import tensorflow as tf\n",
    "import retro\n",
    "\n",
    "import numpy as np\n",
    "import random as r\n",
    "\n",
    "from skimage import transform # to preprocess game frames\n",
    "from skimage.color import rgb2gray # to convert game frames into grayscale\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from collections import deque # to create ordered collections of frames\n",
    "\n",
    "import warnings # ignore warning messages from skiimage during training\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frame Size:  Box(210, 160, 3)\n",
      "Action Space:  6  moves\n",
      "[[1 0 0 0 0 0]\n",
      " [0 1 0 0 0 0]\n",
      " [0 0 1 0 0 0]\n",
      " [0 0 0 1 0 0]\n",
      " [0 0 0 0 1 0]\n",
      " [0 0 0 0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('SpaceInvaders-v0') # make env\n",
    "\n",
    "print(\"Frame Size: \", env.observation_space)\n",
    "print(\"Action Space: \", env.action_space.n, \" moves\")\n",
    "\n",
    "H = 108 # height of preprocessed image\n",
    "W = 84 # width of preprocessed image\n",
    "\n",
    "moves = np.array(np.identity(env.action_space.n, dtype=int).tolist())\n",
    "\n",
    "print(moves)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input size (210, 160, 3)\n"
     ]
    }
   ],
   "source": [
    "for i_episode in range(1):\n",
    "    observation = env.reset()\n",
    "    print(\"input size\", observation.shape)\n",
    "#     for t in range(1):\n",
    "#         env.render()\n",
    "#         print(observation)\n",
    "#         action = env.action_space.sample()\n",
    "#         observation, reward, done, info = env.step(action)\n",
    "#         if done:\n",
    "#             print(\"Episode finished after {} timesteps\".format(t+1))\n",
    "#             break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "STATE_SIZE = [108, 84, 4] # stack dimensions\n",
    "STACK_SIZE = 4 # \"phi length\" - how many frames we consolidate into one training example\n",
    "ACTION_SIZE = env.action_space.n # 6 possible moves\n",
    "LEARNING_RATE = 2.5e-4\n",
    "\n",
    "# training\n",
    "NUM_GAMES = 50 # how many games we will play for training\n",
    "MAX_STEPS = 50000 # max number of moves we'll make each game\n",
    "HIDDEN_UNITS = 512\n",
    "BATCH_SIZE = 64 # num of states we include for each weight update in gradient descent\n",
    "TRAINING = False # when you just want agent to play\n",
    "ENV_RENDER = False # when you want to see env\n",
    "\n",
    "# explore params\n",
    "EXP_START = 1.0\n",
    "EXP_STOP = 0.01\n",
    "DECAY = 1e-5\n",
    "\n",
    "# q le table\n",
    "GAMMA = 0.001 # discount for future rewards in bellman equation\n",
    "\n",
    "# memory\n",
    "LOAD_MEM = BATCH_SIZE # num experiences to be stored in memory during initialization\n",
    "MEMORY_SIZE = int(1e6) # max num experiences stored in memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess raw input from game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def state_to_frame(state):\n",
    "    \n",
    "#     print(\"Input Size: \", state.shape)\n",
    "    \n",
    "    gray_doe = rgb2gray(state)\n",
    "    crop_doe = gray_doe[6:-12, 4:-12]\n",
    "    normalize_doe = crop_doe / 255.0\n",
    "    frame = transform.resize(normalize_doe, [H, W])\n",
    "    \n",
    "#     print(\"Output Size: \", frame.shape)\n",
    "    \n",
    "    return  frame # Shape = 108 height, 84 width | vals = [0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stack_frames(stack_o_frames, state, is_new_game):\n",
    "    \n",
    "    frame = state_to_frame(state)\n",
    "    \n",
    "    if is_new_game: # if new game, clear stack from last game\n",
    "        \n",
    "        for i in range(STACK_SIZE): # in new game, start with copies of OG frame\n",
    "            stack_o_frames.append(frame)\n",
    "            \n",
    "        \n",
    "#         print(\"Stack Size: \", len(stack_o_frames))\n",
    "        \n",
    "#         print(\"State Size: \", state.shape)\n",
    "        \n",
    "    else: # if we already have stack, append latest frame\n",
    "        \n",
    "        stack_o_frames.append(frame) # deque automatically removes oldest frame\n",
    "        \n",
    "    state = np.stack(stack_o_frames, axis=2) # input to neural net, shape = (108, 84, 4)\n",
    "       \n",
    "#     print(\"Snapshot: \", state[:5])\n",
    "        \n",
    "    return stack_o_frames, state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN:\n",
    "    def __init__(self, name=\"DQN\"):\n",
    "        \n",
    "        with tf.variable_scope(name): # create model skeleton, using placeholders for data to be fed in\n",
    "            \n",
    "            self.inputs_ = tf.placeholder(dtype=tf.float32, \n",
    "                                          shape=[None, *STATE_SIZE], \n",
    "                                          # *STATE_SIZE means we take each element of STATE_SIZE and input sequentially\n",
    "                                          # ie. [None, 110, 84, 4] instead of [None, [110, 84, 4]]\n",
    "                                          name=\"inputs\")\n",
    "            \n",
    "            self.actions_ = tf.placeholder(dtype=tf.float32,\n",
    "                                           shape=[None, ACTION_SIZE],\n",
    "                                           name=\"actions\")\n",
    "            \n",
    "            # Target QVal = Reward (state | actions) + discount * QVal (new_state | total_possible_actions)\n",
    "            self.target_q = tf.placeholder(dtype=tf.float32,\n",
    "                                           shape=[None],\n",
    "                                           name=\"target_q\")\n",
    "            \n",
    "            # First Convolution\n",
    "            self.conv1 = tf.layers.conv2d(inputs=self.inputs_,\n",
    "                                          filters=32,\n",
    "                                          kernel_size=[8,8],\n",
    "                                          strides=[4,4],\n",
    "                                          padding=\"VALID\",\n",
    "                                          kernel_initializer=tf.contrib.layers.xavier_initializer_conv2d(),\n",
    "                                          name = \"conv1\")\n",
    "            \n",
    "            # Output size = (Wâˆ’K+2P)/S+1\n",
    "            # height = (108 - 8 + 2*0) / 4 + 1= 26\n",
    "            # width = (84 - 8) / 4 + 1= 20\n",
    "            \n",
    "            self.conv1_shape = self.conv1.get_shape()\n",
    "            \n",
    "            self.conv1_out = tf.nn.elu(self.conv1, name=\"conv1_out\") # shape = (batch_size, 26 ht, 20 wd, 32 filters)\n",
    "            \n",
    "            # Second Convolution\n",
    "            self.conv2 = tf.layers.conv2d(inputs=self.conv1_out,\n",
    "                                          filters=64,\n",
    "                                          kernel_size=[4,4],\n",
    "                                          strides=[2,2],\n",
    "                                          padding=\"VALID\",\n",
    "                                          kernel_initializer=tf.contrib.layers.xavier_initializer_conv2d(),\n",
    "                                          name = \"conv2\")\n",
    "            \n",
    "            self.conv2_shape = self.conv2.get_shape()\n",
    "            \n",
    "            self.conv2_out = tf.nn.elu(self.conv2, name=\"conv2_out\") # shape = (batch_size, 12 ht, 9 wd, 64 filters)\n",
    "            \n",
    "            # Third Convolution\n",
    "            self.conv3 = tf.layers.conv2d(inputs=self.conv2_out,\n",
    "                                          filters=64,\n",
    "                                          kernel_size=[3,3],\n",
    "                                          strides=[2,2],\n",
    "                                          padding=\"VALID\",\n",
    "                                          kernel_initializer=tf.contrib.layers.xavier_initializer_conv2d(),\n",
    "                                          name = \"conv3\")\n",
    "            \n",
    "            self.conv3_shape = self.conv2.get_shape()\n",
    "            \n",
    "            self.conv3_out = tf.nn.elu(self.conv3, name=\"conv3_out\") # shape = (batch size, 12 ht, 9 wd, 64 filters)\n",
    "            \n",
    "            # Flat\n",
    "            self.flat = tf.layers.flatten(self.conv3_out)\n",
    "            \n",
    "            self.flat_shape = self.flat.get_shape()\n",
    "            \n",
    "            # Hidden Layer\n",
    "            self.fully_connected = tf.layers.dense(inputs=self.flat,\n",
    "                                                   units=HIDDEN_UNITS,\n",
    "                                                   activation=tf.nn.elu,\n",
    "                                                   kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                                   name=\"fc1\")\n",
    "            \n",
    "            self.fully_connected_shape = self.fully_connected.get_shape()\n",
    "            \n",
    "            # Linear\n",
    "            self.output = tf.layers.dense(inputs=self.fully_connected,\n",
    "                                          units=ACTION_SIZE,\n",
    "                                          kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                          activation=None)\n",
    "            \n",
    "            self.output_shape = self.output.get_shape()\n",
    "            \n",
    "            # q val\n",
    "            self.q = tf.reduce_sum(tf.multiply(self.output, self.actions_))\n",
    "        \n",
    "            # loss = (discounted_q_vals_in_future - q_val_of_move)^2\n",
    "            self.loss = tf.reduce_mean(tf.square(self.target_q - self.q))\n",
    "            \n",
    "            # backprop\n",
    "            self.optimizer = tf.train.AdamOptimizer(LEARNING_RATE).minimize(self.loss)                    \n",
    "            \n",
    "    def print_shapes(self):\n",
    "        print(\"c1 \", self.conv1_shape)\n",
    "        print(\"c2 \", self.conv2_shape)\n",
    "        print(\"c3 \", self.conv3_shape)\n",
    "        print(\"flat \", self.fully_connected_shape)\n",
    "        print(\"fully_connected \", self.fully_connected_shape)\n",
    "        print(\"output \", self.output_shape)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From <ipython-input-8-1c3f4fc01f22>:28: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.conv2d instead.\n",
      "WARNING:tensorflow:From /Users/katz/Applications/anaconda3/envs/rl/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From <ipython-input-8-1c3f4fc01f22>:65: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.flatten instead.\n",
      "WARNING:tensorflow:From <ipython-input-8-1c3f4fc01f22>:74: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dense instead.\n",
      "c1  (?, 26, 20, 32)\n",
      "c2  (?, 12, 9, 64)\n",
      "c3  (?, 12, 9, 64)\n",
      "flat  (?, 512)\n",
      "fully_connected  (?, 512)\n",
      "output  (?, 6)\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "DQN = DQN()\n",
    "\n",
    "DQN.print_shapes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Memory module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Memory():\n",
    "    def __init__(self):\n",
    "        self.buffer = deque(maxlen = MEMORY_SIZE)\n",
    "        \n",
    "    def add(self, experience):\n",
    "        self.buffer.append(experience)\n",
    "        \n",
    "    def sample(self):\n",
    "        buffer_size = len(self.buffer)\n",
    "        index = np.random_choice(np.arrange(buffer_size),\n",
    "                                 size=BATCH_SIZE,\n",
    "                                 replace=False)\n",
    "        \n",
    "        return [self.buffer[i] for i in index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiences Loaded:  64\n"
     ]
    }
   ],
   "source": [
    "memory = Memory()\n",
    "\n",
    "state = env.reset() # reset game\n",
    "stack = deque([np.zeros((H,W), dtype=np.int) for i in range(STACK_SIZE)], maxlen=STACK_SIZE) # load deque with zeros\n",
    "\n",
    "# load first state, create first stack\n",
    "stack, state = stack_frames(stack, state, is_new_game=True)\n",
    "\n",
    "for i in range(LOAD_MEM):\n",
    "    \n",
    "    choice = r.randint(1, len(moves)) - 1 # random index\n",
    "    action = moves[choice] # chosen index\n",
    "    next_state, reward, done, _logs_ = env.step(choice) # act\n",
    "    \n",
    "    env.render()\n",
    "    \n",
    "    stack, next_state = stack_frames(stack, next_state, is_new_game=False) # save next state\n",
    "    \n",
    "    if done: # when we lose, create a 0-matrix for next_state\n",
    "        next_state = np.zeros(state.shape)  # Add experience to memory\n",
    "    \n",
    "    memory.add((state, action, reward, next_state, done)) # add experience to memory \n",
    "    \n",
    "    state = next_state\n",
    "    \n",
    "    if done: # start new game\n",
    "        state = env.reset()\n",
    "        stack, state = stack_frames(stack, state, is_new_game=True) \n",
    "\n",
    "print(\"Experiences Loaded: \", len(memory.buffer))\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name Explore Prob is illegal; using Explore_Prob instead.\n"
     ]
    }
   ],
   "source": [
    "writer = tf.summary.FileWriter(\"./tensorboard/dqn/1\") # write\n",
    "\n",
    "tf.summary.scalar(\"Loss\", DQN.loss) \n",
    "\n",
    "write_op = tf.summary.merge_all() # merge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def move(EXP_START, EXP_STOP, DECAY, training_step, state, moves):\n",
    "    \n",
    "    adventure = r.uniform(0, 1)\n",
    "\n",
    "    explore_prob = EXP_STOP - (EXP_START - EXP_STOP) * np.exp(-DECAY_RATE * training_step)\n",
    "\n",
    "    if explore_prob > adventure: # explore randomly\n",
    "        choice = r.randint(1, len(moves)) - 1\n",
    "        action = moves[choice]\n",
    "        \n",
    "    else: # use dqn\n",
    "        dq_table = sess.run(DQN.output, feed_dict={DQN.inputs_:state.reshape((1, *state.shape))}) # run graph with state as input\n",
    "        choice = np.argmax(dq_table) # pick move with highest predicted q val\n",
    "        action = moves[choice]\n",
    "        \n",
    "    return action, explore_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
