{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import tensorflow as tf\n",
    "import retro\n",
    "\n",
    "import numpy as np\n",
    "import random as r\n",
    "\n",
    "from skimage import transform # to preprocess game frames\n",
    "from skimage.color import rgb2gray # to convert game frames into grayscale\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import imshow\n",
    "from PIL import Image\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv2D\n",
    "\n",
    "from collections import deque # to create ordered collections of frames\n",
    "\n",
    "import warnings # ignore warning messages from skiimage during training\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frame Size:  Box(210, 160, 3)\n",
      "Action Space:  6  moves\n",
      "\n",
      "This is what the agent sees: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x12be04dd8>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "env = gym.make('SpaceInvaders-v0') # make env\n",
    "\n",
    "print(\"Frame Size: \", env.observation_space)\n",
    "print(\"Action Space: \", env.action_space.n, \" moves\")\n",
    "\n",
    "UP = 26\n",
    "DOWN = -14\n",
    "LEFT = 14\n",
    "RIGHT = -14\n",
    "\n",
    "H = 210 - (UP - DOWN) # height of preprocessed image\n",
    "W = 160 - (LEFT - RIGHT) # width of preprocessed image\n",
    "\n",
    "obs = env.reset()[UP:DOWN, LEFT:RIGHT]\n",
    "obs = obs[:, :, 0]\n",
    "\n",
    "%matplotlib inline\n",
    "img = Image.fromarray(obs, 'L')\n",
    "\n",
    "print(\"\\nThis is what the agent sees: \")\n",
    "imshow(np.asarray(img))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "NUM_FRAMES = 4 # \"phi length\" - how many frames we consolidate into one training example\n",
    "X_DIMS = (H, W, NUM_FRAMES)# input dimensions to NN \n",
    "NUM_POSSIBLE_ACTIONS = env.action_space.n # 6 possible moves\n",
    "LEARNING_RATE = 2.5e-4\n",
    "\n",
    "# training\n",
    "NUM_GAMES = 500 # how many games we will play for training\n",
    "HIDDEN_UNITS = 512\n",
    "BATCH_SIZE = 24 # num of states we include for each weight update in gradient descent\n",
    "\n",
    "# explore params\n",
    "EXPLORE = 5e-6\n",
    "\n",
    "# q le table\n",
    "GAMMA = 0.001 # discount for future rewards in bellman equation\n",
    "\n",
    "# memory\n",
    "# LOAD_MEM = 64 # num experiences to be stored in memory during initialization\n",
    "# MEMORY_SIZE = int(1e6) # max num experiences stored in memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess raw input from game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_state(state):\n",
    "    \n",
    "#     print(\"Input Size: \", state.shape)\n",
    "    \n",
    "    gray_doe = rgb2gray(state)\n",
    "    crop_doe = gray_doe[UP:DOWN, LEFT:RIGHT]\n",
    "    observation = crop_doe / 255.0\n",
    "    \n",
    "#     print(\"Output Size: \", frame.shape)\n",
    "    \n",
    "    return observation # Shape = 172 height, 132 width | vals = [0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_observation(observations, state, game_step):\n",
    "    \n",
    "    obs = process_state(state)\n",
    "    \n",
    "    if game_step == 0: # new game\n",
    "        for image in range(len(observations)):\n",
    "            observations.append(obs)\n",
    "    \n",
    "    else:\n",
    "        observations.append(obs)\n",
    "    \n",
    "    return observations\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_move(model, observations, moves, training_step):\n",
    "    \n",
    "    adventure = r.uniform(0, 1)\n",
    "\n",
    "    explore_prob = min(EXPLORE * training_step, 0.9)\n",
    "\n",
    "    if explore_prob < adventure: # explore randomly\n",
    "        choice = r.randint(1, len(moves)) - 1\n",
    "        action = moves[choice]\n",
    "        \n",
    "    else: # use dqn\n",
    "        obs = np.stack(observations, axis=2)\n",
    "        dq_table = model.predict(obs)\n",
    "        choice = np.argmax(dq_table) # pick move with highest predicted q val\n",
    "        action = moves[choice]\n",
    "        \n",
    "    return action, explore_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class dqn:\n",
    "    def __init__(self, name='dqn'):\n",
    "        \n",
    "        self.model = Sequential()\n",
    "        \n",
    "        self.model.add(Conv2D(filters=16, \n",
    "                              kernel_size=(2, 2), \n",
    "                              strides=4,\n",
    "                              activation='relu', \n",
    "                              input_shape=(X_DIMS)))\n",
    "        \n",
    "        self.model.add(Conv2D(filters=32, \n",
    "                              kernel_size=(2, 2),\n",
    "                              strides=2,\n",
    "                              activation='relu'))\n",
    "        \n",
    "        self.model.add(Conv2D(filters=64, \n",
    "                              kernel_size=(2, 2),\n",
    "                              strides=1,\n",
    "                              activation='relu'))\n",
    "        \n",
    "        self.model.add(Flatten())\n",
    "        \n",
    "        self.model.add(Dense(64, activation='relu'))\n",
    "        \n",
    "        self.model.add(Dense(NUM_POSSIBLE_ACTIONS, activation='softmax'))\n",
    "        \n",
    "        self.model.compile(loss='mse', optimizer='adam', metrics=['accuracy'])\n",
    "        \n",
    "        print(self.model.summary())\n",
    "    \n",
    "    \n",
    "#     def backprop(self, loss_fn, q_predict, q_target, optimizer='adam'):\n",
    "#         return self.model.compile(optimizer=optimizer, loss=loss_fn(q_predict, q_target))\n",
    "    \n",
    "    def predict(self, x):\n",
    "        return self.model.predict(np.expand_dims(x, axis=0))\n",
    "    \n",
    "    def train_on_batch(self, x, y):\n",
    "        return self.model.train_on_batch(x, y)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn(mb, model):\n",
    "    \n",
    "    x_shape = (len(mb),) + X_DIMS\n",
    "    x = np.zeros(x_shape) \n",
    "    y = np.zeros((len(mb), NUM_POSSIBLE_ACTIONS))\n",
    "#     print(x.shape)\n",
    "#     print(y.shape)\n",
    "    \n",
    "    for exp in range(len(mb)):\n",
    "        obs = np.stack(mb[exp][0], axis=2)\n",
    "        action = mb[exp][1]\n",
    "        reward = mb[exp][2]\n",
    "        new_obs = np.stack(mb[exp][3], axis=2)\n",
    "        done = mb[exp][4]\n",
    "        \n",
    "        x[exp] = obs\n",
    "        y[exp] = model.predict(obs)\n",
    "        Q_future = model.predict(new_obs)\n",
    "        \n",
    "        if done:\n",
    "            y[exp, action] = reward\n",
    "        else:\n",
    "            y[exp, action] = reward + GAMMA * np.max(Q_future)\n",
    "        \n",
    "        loss = model.train_on_batch(x, y)\n",
    "        \n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 43, 33, 16)        272       \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 21, 16, 32)        2080      \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 20, 15, 64)        8256      \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 19200)             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 64)                1228864   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 6)                 390       \n",
      "=================================================================\n",
      "Total params: 1,239,862\n",
      "Trainable params: 1,239,862\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "GAME 0 FINISHED || Total Score: 225.0 || Avg. Loss: 0.1193699985742569 || Explore Prob: 0.0044150000000000005\n",
      "GAME 5 FINISHED || Total Score: 30.0 || Avg. Loss: 0.03313999995589256 || Explore Prob: 0.018555000000000002\n",
      "GAME 10 FINISHED || Total Score: 225.0 || Avg. Loss: 0.03077000007033348 || Explore Prob: 0.036805000000000004\n",
      "GAME 15 FINISHED || Total Score: 90.0 || Avg. Loss: 0.3259100019931793 || Explore Prob: 0.053935000000000004\n",
      "GAME 20 FINISHED || Total Score: 110.0 || Avg. Loss: 0.030079999938607216 || Explore Prob: 0.07423\n",
      "GAME 25 FINISHED || Total Score: 80.0 || Avg. Loss: 0.028689999133348465 || Explore Prob: 0.08947000000000001\n",
      "GAME 30 FINISHED || Total Score: 105.0 || Avg. Loss: 0.07558000087738037 || Explore Prob: 0.10495000000000002\n",
      "GAME 35 FINISHED || Total Score: 210.0 || Avg. Loss: 0.06910999864339828 || Explore Prob: 0.12136000000000001\n",
      "GAME 40 FINISHED || Total Score: 215.0 || Avg. Loss: 0.047290001064538956 || Explore Prob: 0.14024\n",
      "GAME 45 FINISHED || Total Score: 210.0 || Avg. Loss: 0.029030000790953636 || Explore Prob: 0.16175\n",
      "GAME 50 FINISHED || Total Score: 160.0 || Avg. Loss: 0.04100000113248825 || Explore Prob: 0.17987\n",
      "GAME 55 FINISHED || Total Score: 225.0 || Avg. Loss: 0.029249999672174454 || Explore Prob: 0.196755\n",
      "GAME 60 FINISHED || Total Score: 30.0 || Avg. Loss: 0.03328000009059906 || Explore Prob: 0.21274\n",
      "GAME 65 FINISHED || Total Score: 75.0 || Avg. Loss: 0.028449999168515205 || Explore Prob: 0.229085\n",
      "GAME 70 FINISHED || Total Score: 135.0 || Avg. Loss: 0.027079999446868896 || Explore Prob: 0.250865\n",
      "GAME 75 FINISHED || Total Score: 140.0 || Avg. Loss: 0.03553000092506409 || Explore Prob: 0.26972\n",
      "GAME 80 FINISHED || Total Score: 180.0 || Avg. Loss: 0.03376999869942665 || Explore Prob: 0.28703\n",
      "GAME 85 FINISHED || Total Score: 410.0 || Avg. Loss: 0.3026700019836426 || Explore Prob: 0.311555\n",
      "GAME 90 FINISHED || Total Score: 140.0 || Avg. Loss: 0.027300000190734863 || Explore Prob: 0.330185\n",
      "GAME 95 FINISHED || Total Score: 75.0 || Avg. Loss: 0.02198999933898449 || Explore Prob: 0.34808000000000006\n",
      "GAME 100 FINISHED || Total Score: 105.0 || Avg. Loss: 0.018309999257326126 || Explore Prob: 0.36550000000000005\n",
      "GAME 105 FINISHED || Total Score: 145.0 || Avg. Loss: 0.09869000315666199 || Explore Prob: 0.38624\n",
      "GAME 110 FINISHED || Total Score: 5.0 || Avg. Loss: 0.025469999760389328 || Explore Prob: 0.407575\n",
      "GAME 115 FINISHED || Total Score: 115.0 || Avg. Loss: 0.059450000524520874 || Explore Prob: 0.425615\n",
      "GAME 120 FINISHED || Total Score: 75.0 || Avg. Loss: 0.022180000320076942 || Explore Prob: 0.44667500000000004\n",
      "GAME 125 FINISHED || Total Score: 180.0 || Avg. Loss: 0.023250000551342964 || Explore Prob: 0.46731000000000006\n",
      "GAME 130 FINISHED || Total Score: 180.0 || Avg. Loss: 0.023509999737143517 || Explore Prob: 0.48833000000000004\n",
      "GAME 135 FINISHED || Total Score: 105.0 || Avg. Loss: 0.02143000066280365 || Explore Prob: 0.50865\n",
      "GAME 140 FINISHED || Total Score: 135.0 || Avg. Loss: 0.03369000181555748 || Explore Prob: 0.5293\n",
      "GAME 145 FINISHED || Total Score: 140.0 || Avg. Loss: 0.020579999312758446 || Explore Prob: 0.5465800000000001\n",
      "GAME 150 FINISHED || Total Score: 180.0 || Avg. Loss: 0.020549999549984932 || Explore Prob: 0.5651900000000001\n",
      "GAME 155 FINISHED || Total Score: 210.0 || Avg. Loss: 0.07981999963521957 || Explore Prob: 0.5842550000000001\n",
      "GAME 160 FINISHED || Total Score: 180.0 || Avg. Loss: 0.020759999752044678 || Explore Prob: 0.603615\n",
      "GAME 165 FINISHED || Total Score: 200.0 || Avg. Loss: 0.022220000624656677 || Explore Prob: 0.6222300000000001\n",
      "GAME 170 FINISHED || Total Score: 540.0 || Avg. Loss: 0.0815500020980835 || Explore Prob: 0.65222\n",
      "GAME 175 FINISHED || Total Score: 135.0 || Avg. Loss: 0.019360000267624855 || Explore Prob: 0.6721950000000001\n",
      "GAME 180 FINISHED || Total Score: 90.0 || Avg. Loss: 0.06938999891281128 || Explore Prob: 0.6921250000000001\n",
      "GAME 185 FINISHED || Total Score: 180.0 || Avg. Loss: 0.4997200071811676 || Explore Prob: 0.715925\n",
      "GAME 190 FINISHED || Total Score: 195.0 || Avg. Loss: 0.13237999379634857 || Explore Prob: 0.734225\n",
      "GAME 195 FINISHED || Total Score: 235.0 || Avg. Loss: 0.08668000251054764 || Explore Prob: 0.76046\n",
      "GAME 200 FINISHED || Total Score: 180.0 || Avg. Loss: 0.031929999589920044 || Explore Prob: 0.7768100000000001\n",
      "GAME 205 FINISHED || Total Score: 295.0 || Avg. Loss: 0.018859999254345894 || Explore Prob: 0.8044100000000001\n",
      "GAME 210 FINISHED || Total Score: 155.0 || Avg. Loss: 0.1221499964594841 || Explore Prob: 0.82262\n",
      "GAME 215 FINISHED || Total Score: 105.0 || Avg. Loss: 0.01489999983459711 || Explore Prob: 0.8434\n",
      "GAME 220 FINISHED || Total Score: 80.0 || Avg. Loss: 0.4944700002670288 || Explore Prob: 0.8600850000000001\n",
      "GAME 225 FINISHED || Total Score: 105.0 || Avg. Loss: 0.0647599995136261 || Explore Prob: 0.8797800000000001\n",
      "GAME 230 FINISHED || Total Score: 30.0 || Avg. Loss: 0.014209999702870846 || Explore Prob: 0.8948200000000001\n",
      "GAME 235 FINISHED || Total Score: 50.0 || Avg. Loss: 0.014960000291466713 || Explore Prob: 0.9\n",
      "GAME 240 FINISHED || Total Score: 30.0 || Avg. Loss: 0.016009999439120293 || Explore Prob: 0.9\n",
      "GAME 245 FINISHED || Total Score: 100.0 || Avg. Loss: 0.014670000411570072 || Explore Prob: 0.9\n",
      "GAME 250 FINISHED || Total Score: 60.0 || Avg. Loss: 0.11266999691724777 || Explore Prob: 0.9\n",
      "GAME 255 FINISHED || Total Score: 50.0 || Avg. Loss: 0.09797000139951706 || Explore Prob: 0.9\n",
      "GAME 260 FINISHED || Total Score: 120.0 || Avg. Loss: 0.018689999356865883 || Explore Prob: 0.9\n",
      "GAME 265 FINISHED || Total Score: 115.0 || Avg. Loss: 0.017090000212192535 || Explore Prob: 0.9\n",
      "GAME 270 FINISHED || Total Score: 225.0 || Avg. Loss: 0.10337000340223312 || Explore Prob: 0.9\n",
      "GAME 275 FINISHED || Total Score: 65.0 || Avg. Loss: 0.42684999108314514 || Explore Prob: 0.9\n",
      "GAME 280 FINISHED || Total Score: 275.0 || Avg. Loss: 0.11580999940633774 || Explore Prob: 0.9\n",
      "GAME 285 FINISHED || Total Score: 100.0 || Avg. Loss: 0.01498000044375658 || Explore Prob: 0.9\n",
      "GAME 290 FINISHED || Total Score: 185.0 || Avg. Loss: 0.07055000215768814 || Explore Prob: 0.9\n",
      "GAME 295 FINISHED || Total Score: 170.0 || Avg. Loss: 0.016610000282526016 || Explore Prob: 0.9\n",
      "GAME 300 FINISHED || Total Score: 155.0 || Avg. Loss: 0.05990999937057495 || Explore Prob: 0.9\n",
      "GAME 305 FINISHED || Total Score: 180.0 || Avg. Loss: 0.025469999760389328 || Explore Prob: 0.9\n",
      "GAME 310 FINISHED || Total Score: 180.0 || Avg. Loss: 0.013410000130534172 || Explore Prob: 0.9\n",
      "GAME 315 FINISHED || Total Score: 125.0 || Avg. Loss: 0.01513999979943037 || Explore Prob: 0.9\n",
      "GAME 320 FINISHED || Total Score: 135.0 || Avg. Loss: 0.014059999957680702 || Explore Prob: 0.9\n",
      "GAME 325 FINISHED || Total Score: 115.0 || Avg. Loss: 0.015010000206530094 || Explore Prob: 0.9\n",
      "GAME 330 FINISHED || Total Score: 230.0 || Avg. Loss: 0.01631000079214573 || Explore Prob: 0.9\n",
      "GAME 335 FINISHED || Total Score: 265.0 || Avg. Loss: 0.014840000309050083 || Explore Prob: 0.9\n",
      "GAME 340 FINISHED || Total Score: 380.0 || Avg. Loss: 0.08263000100851059 || Explore Prob: 0.9\n",
      "GAME 345 FINISHED || Total Score: 180.0 || Avg. Loss: 0.036479998379945755 || Explore Prob: 0.9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GAME 350 FINISHED || Total Score: 135.0 || Avg. Loss: 0.05857999995350838 || Explore Prob: 0.9\n",
      "GAME 355 FINISHED || Total Score: 210.0 || Avg. Loss: 0.014630000106990337 || Explore Prob: 0.9\n",
      "GAME 360 FINISHED || Total Score: 30.0 || Avg. Loss: 0.49480998516082764 || Explore Prob: 0.9\n",
      "GAME 365 FINISHED || Total Score: 95.0 || Avg. Loss: 0.08068999648094177 || Explore Prob: 0.9\n",
      "GAME 370 FINISHED || Total Score: 15.0 || Avg. Loss: 0.4938800036907196 || Explore Prob: 0.9\n",
      "GAME 375 FINISHED || Total Score: 180.0 || Avg. Loss: 0.14358000457286835 || Explore Prob: 0.9\n",
      "GAME 380 FINISHED || Total Score: 820.0 || Avg. Loss: 0.014440000057220459 || Explore Prob: 0.9\n",
      "GAME 385 FINISHED || Total Score: 155.0 || Avg. Loss: 0.015939999371767044 || Explore Prob: 0.9\n",
      "GAME 390 FINISHED || Total Score: 155.0 || Avg. Loss: 0.08556000143289566 || Explore Prob: 0.9\n",
      "GAME 395 FINISHED || Total Score: 30.0 || Avg. Loss: 0.028920000419020653 || Explore Prob: 0.9\n",
      "GAME 400 FINISHED || Total Score: 65.0 || Avg. Loss: 0.0654899999499321 || Explore Prob: 0.9\n",
      "GAME 405 FINISHED || Total Score: 50.0 || Avg. Loss: 0.015650000423192978 || Explore Prob: 0.9\n",
      "GAME 410 FINISHED || Total Score: 50.0 || Avg. Loss: 0.029519999399781227 || Explore Prob: 0.9\n",
      "GAME 415 FINISHED || Total Score: 320.0 || Avg. Loss: 0.1602800041437149 || Explore Prob: 0.9\n",
      "GAME 420 FINISHED || Total Score: 575.0 || Avg. Loss: 0.07632999867200851 || Explore Prob: 0.9\n",
      "GAME 425 FINISHED || Total Score: 245.0 || Avg. Loss: 0.08096999675035477 || Explore Prob: 0.9\n",
      "GAME 430 FINISHED || Total Score: 210.0 || Avg. Loss: 0.08144000172615051 || Explore Prob: 0.9\n",
      "GAME 435 FINISHED || Total Score: 355.0 || Avg. Loss: 0.014709999784827232 || Explore Prob: 0.9\n",
      "GAME 440 FINISHED || Total Score: 60.0 || Avg. Loss: 0.06368999928236008 || Explore Prob: 0.9\n",
      "GAME 445 FINISHED || Total Score: 15.0 || Avg. Loss: 0.4948900043964386 || Explore Prob: 0.9\n",
      "GAME 450 FINISHED || Total Score: 5.0 || Avg. Loss: 0.013410000130534172 || Explore Prob: 0.9\n",
      "GAME 455 FINISHED || Total Score: 30.0 || Avg. Loss: 0.015860000625252724 || Explore Prob: 0.9\n",
      "GAME 460 FINISHED || Total Score: 50.0 || Avg. Loss: 0.014630000106990337 || Explore Prob: 0.9\n",
      "GAME 465 FINISHED || Total Score: 50.0 || Avg. Loss: 0.10454999655485153 || Explore Prob: 0.9\n",
      "GAME 470 FINISHED || Total Score: 410.0 || Avg. Loss: 0.015150000341236591 || Explore Prob: 0.9\n",
      "GAME 475 FINISHED || Total Score: 260.0 || Avg. Loss: 0.0175199992954731 || Explore Prob: 0.9\n",
      "GAME 480 FINISHED || Total Score: 180.0 || Avg. Loss: 0.025119999423623085 || Explore Prob: 0.9\n",
      "GAME 485 FINISHED || Total Score: 210.0 || Avg. Loss: 0.015309999696910381 || Explore Prob: 0.9\n",
      "GAME 490 FINISHED || Total Score: 180.0 || Avg. Loss: 0.014059999957680702 || Explore Prob: 0.9\n",
      "GAME 495 FINISHED || Total Score: 95.0 || Avg. Loss: 0.10716000199317932 || Explore Prob: 0.9\n"
     ]
    }
   ],
   "source": [
    "model = dqn()\n",
    "training_step = 1\n",
    "moves = list(range(0, NUM_POSSIBLE_ACTIONS))\n",
    "\n",
    "observations = deque([np.zeros((H,W), dtype=np.float) for i in range(NUM_FRAMES)], maxlen=NUM_FRAMES)\n",
    "memory = list()\n",
    "\n",
    "for game in range(NUM_GAMES):\n",
    "    \n",
    "    game_step = 0\n",
    "    game_rewards = []\n",
    "    game_losses = []\n",
    "    \n",
    "    state = env.reset()\n",
    "    observations = add_observation(observations, state, game_step)\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        \n",
    "        action, explore_prob = get_move(model, observations, moves, training_step)\n",
    "        new_state, reward, done, _ = env.step(action)\n",
    "        game_step += 1\n",
    "        game_rewards.append(reward)\n",
    "        \n",
    "#         env.render()\n",
    "        \n",
    "        new_observations = add_observation(observations, new_state, game_step) \n",
    "        memory.append((observations, action, reward, new_observations, done))\n",
    "        observations = new_observations\n",
    "        training_step += 1\n",
    "        \n",
    "        if training_step % BATCH_SIZE == 0:\n",
    "            minibatch = memory[-BATCH_SIZE:]\n",
    "            loss = learn(minibatch, model)\n",
    "            game_losses.append(loss)\n",
    "            \n",
    "    total_reward = np.sum(game_rewards)\n",
    "    avg_loss = np.round(np.mean(game_losses), decimals=5)\n",
    "    \n",
    "    if game % 5 == 0:\n",
    "        print(\"GAME {} FINISHED || Total Score: {} || Avg. Loss: {} || Explore Prob: {}\".format(game, total_reward, avg_loss, explore_prob))\n",
    "        \n",
    "        \n",
    "# create \n",
    "# move (read q table)\n",
    "# observe reward\n",
    "# calculate loss\n",
    "# backprop\n",
    "# update q table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for game in range(5):\n",
    "    \n",
    "    game_step = 0\n",
    "    game_rewards = []\n",
    "    game_losses = []\n",
    "    \n",
    "    state = env.reset()\n",
    "    observations = add_observation(observations, state, game_step)\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        \n",
    "        action, explore_prob = get_move(model, observations, moves, training_step)\n",
    "        new_state, reward, done, _ = env.step(action)\n",
    "        game_step += 1\n",
    "        game_rewards.append(reward)\n",
    "        \n",
    "        env.render()\n",
    "        \n",
    "        new_observations = add_observation(observations, new_state, game_step) \n",
    "        memory.append((observations, action, reward, new_observations, done))\n",
    "        observations = new_observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Memory module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAINING == True:\n",
    "    \n",
    "    model = dqn()\n",
    "    training_step = 0\n",
    "    \n",
    "    for game in range (NUM_GAMES + 1):\n",
    "        \n",
    "        game_step = 0\n",
    "        game_rewards = []\n",
    "        state = env.reset()\n",
    "        \n",
    "        stack, state = stack_frames(stack, state, is_new_game=True)\n",
    "        \n",
    "        while not done:\n",
    "            \n",
    "            game_step += 1\n",
    "            training_step += 1\n",
    "            \n",
    "            action, explore_prob = move(model, state, moves, training_step) # choose move\n",
    "                \n",
    "            next_state, reward, done, _logs_ = env.step(np.argmax(action)) # act\n",
    "            \n",
    "            if ENV_RENDER: # show games\n",
    "                env.render()\n",
    "                \n",
    "            game_rewards.append(reward)\n",
    "            \n",
    "            if done: # we've lost\n",
    "                next_state = np.zeros(state.shape)\n",
    "                stack, next_state = stack_frames(stack, next_state, is_new_game=False) \n",
    "                game_step = MAX_STEPS\n",
    "                total_reward = np.sum(game_rewards)\n",
    "                memory.add((state, action, reward, next_state, done))\n",
    "\n",
    "                print('Game: {}'.format(game), \n",
    "                      'Score: {}'.format(round(total_reward, 3)),\n",
    "                      'Explore Prob: {}'.format(round(explore_prob, 3)),\n",
    "                      'Training Loss {}'.format(round(loss, 4)))\n",
    "        \n",
    "            else: # we're still alive!\n",
    "                stack, next_state = stack_frames(stack, next_state, is_new_game=False)\n",
    "                memory.add((state, action, reward, next_state, done))\n",
    "                state = next_state\n",
    "                \n",
    "            train(model, memory)\n",
    "                \n",
    "            if game % 5 == 0: # save model every five games\n",
    "                print('Checkpoint')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Memory():\n",
    "    def __init__(self):\n",
    "        self.buffer = deque(maxlen = MEMORY_SIZE)\n",
    "        \n",
    "    def add(self, experience):\n",
    "        self.buffer.append(experience)\n",
    "        \n",
    "    def sample(self):\n",
    "        buffer_size = len(self.buffer)\n",
    "        index = np.random.choice(np.arange(buffer_size),\n",
    "                                 size=BATCH_SIZE,\n",
    "                                 replace=False)\n",
    "        \n",
    "        return [self.buffer[i] for i in index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_memory():\n",
    "    \n",
    "    memory = Memory()\n",
    "    state = env.reset() # reset game\n",
    "    obs = []\n",
    "    \n",
    "    # initialize observations\n",
    "    obs = add_observation(observations, state, is_new_game=True)\n",
    "\n",
    "    for i in range(LOAD_MEM):\n",
    "\n",
    "        choice = r.randint(1, len(moves)) - 1 # random index\n",
    "        action = moves[choice] # chosen index\n",
    "        new_state, reward, done, _ = env.step(choice) # act\n",
    "\n",
    "        env.render()\n",
    "\n",
    "        obs = add_observations(obs, new_state, is_new_game=False) # save next state\n",
    "\n",
    "        if done: # when we lose, create a 0-matrix for next_state\n",
    "            next_state = np.zeros(state.shape)\n",
    "\n",
    "        memory.add((state, action, reward, next_state, done)) # add experience to memory \n",
    "\n",
    "        state = new_state\n",
    "\n",
    "        if done: # start new game\n",
    "            state = env.reset()\n",
    "            obs = add_observation(obs, state, is_new_game=True) \n",
    "\n",
    "    print(\"Experiences Loaded: \", len(memory.buffer))\n",
    "\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(q_predict, q_target):\n",
    "    return tf.reduce_mean(tf.square(q_target - q_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, memory):\n",
    "\n",
    "    batch = memory.sample()\n",
    "\n",
    "    # retrieve minibatches from memory\n",
    "    states_mini = np.array([states[0] for states in batch], ndmin=3)\n",
    "    actions_mini = np.array([actions[1] for actions in batch])\n",
    "    rewards_mini = np.array([rewards[2] for rewards in batch])\n",
    "    new_states_mini = np.array([next_states[3] for next_states in batch], ndmin=3)\n",
    "    dones_mini = np.array([dones[4] for dones in batch])\n",
    "    \n",
    "    q_target = []\n",
    "    \n",
    "    # predict q vals for next state\n",
    "    q_predict = model.predict(next_states_mini)\n",
    "    \n",
    "    for i in range(0, len(batch)): # loop through batches\n",
    "        \n",
    "        gameover = dones_mini[i]\n",
    "        \n",
    "        if gameover: # target_q = reward\n",
    "            q_target.append(rewards_mini[i])\n",
    "        \n",
    "        else: # target_q = reward + gamma * max_q(s', a')\n",
    "            target = rewards_mini[i] + GAMMA * np.max(q_predict[i])\n",
    "            q_target.append(target)\n",
    "            \n",
    "    targets_mini = np.array([target for target in q_target]) # create minibatch of target q vals\n",
    "    \n",
    "    # calculate loss and backprop on minibatches\n",
    "    @tf.function\n",
    "    def train_one_batch()\n",
    "        with tf.GradientTape() as tape:\n",
    "            loss = loss_fn(q_predict, q_target)\n",
    "\n",
    "            grads = tape.gradient(loss, model.trainable_variables)\n",
    "            optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "        return loss\n",
    "    \n",
    "    model.backprop(q_predict, q_target)\n",
    "    \n",
    "    pass "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saver = tf.train.Saver() \n",
    "\n",
    "if TRAINING == True:\n",
    "    \n",
    "    with tf.Session() as sesh:\n",
    "        \n",
    "        tf.global_variables_initializer().run()\n",
    "        \n",
    "        training_step = 0\n",
    "        \n",
    "        for game in range(NUM_GAMES+1): # play games\n",
    "            \n",
    "            # initialize\n",
    "            game_step = 0\n",
    "            game_rewards = []\n",
    "            state = env.reset()\n",
    "            \n",
    "            stack, state = stack_frames(stack, state, is_new_game=True)\n",
    "            \n",
    "            while game_step < MAX_STEPS:\n",
    "                \n",
    "                game_step += 1\n",
    "                training_step += 1\n",
    "                \n",
    "                action, explore_prob = move(state, moves, training_step) # choose move\n",
    "                \n",
    "                next_state, reward, done, _logs_ = env.step(np.argmax(action)) # act\n",
    "                \n",
    "                if ENV_RENDER: # show games\n",
    "                    env.render()\n",
    "                    \n",
    "                game_rewards.append(reward) # track reward\n",
    "                \n",
    "                if done: # we've lost\n",
    "                    next_state = np.zeros(state.shape)\n",
    "                    stack, next_state = stack_frames(stack, next_state, is_new_game=False) \n",
    "                    game_step = MAX_STEPS\n",
    "                    total_reward = np.sum(game_rewards)\n",
    "                    memory.add((state, action, reward, next_state, done))\n",
    "                    \n",
    "                    print('Game: {}'.format(game), \n",
    "                          'Score: {}'.format(round(total_reward, 3)),\n",
    "                          'Explore Prob: {}'.format(round(explore_prob, 3)),\n",
    "                          'Training Loss {}'.format(round(loss, 4)))\n",
    "        \n",
    "                else: # we're still alive!\n",
    "                    stack, next_state = stack_frames(stack, next_state, is_new_game=False)\n",
    "                    memory.add((state, action, reward, next_state, done))\n",
    "                    state = next_state\n",
    "                \n",
    "                loss = learn(DQN, memory, sesh)\n",
    "                \n",
    "            if game % 5 == 0: # save model every five games\n",
    "                save_path = saver.save(sesh, './models/model.cpkt')\n",
    "                print('Checkpoint')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%bash\n",
    "# tensorboard --logdir='./tensorboard/dqn'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sesh:\n",
    "    total_test_rewards = []\n",
    "    \n",
    "    saver.restore(sesh, \"./models/model.cpkt\")\n",
    "    \n",
    "    for game in range(3):\n",
    "        total_rewards = 0\n",
    "        state = env.reset()\n",
    "        stack, state = stack_frames(stack, state, is_new_game=True)\n",
    "        \n",
    "        print(\"****************************************************\")\n",
    "        print(\"GAME \", game)\n",
    "        \n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            state = state.reshape((1, *STATE_SIZE))\n",
    "            q_vals = sesh.run(DQN.output, feed_dict={DQN.inputs_:state})\n",
    "            \n",
    "            next_state, reward, done, _logs_ = env.step(np.argmax(q_vals))\n",
    "            \n",
    "            env.render()\n",
    "            \n",
    "            total_rewards += reward\n",
    "            \n",
    "            if done:\n",
    "                print(\"Score: \", total_rewards)\n",
    "                total_test_rewards.append(total_rewards)\n",
    "            \n",
    "            stack, next_state = stack_frames(stack, next_state, is_new_game=False)\n",
    "            state = next_state\n",
    "            \n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = tf.summary.FileWriter(\"./tensorboard/dqn/1\") # write\n",
    "\n",
    "tf.summary.scalar(\"Loss\", DQN.loss) \n",
    "\n",
    "write_op = tf.summary.merge_all() # merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN:\n",
    "    def __init__(self, name=\"DQN\"):\n",
    "        \n",
    "        with tf.variable_scope(name): # create model skeleton, using placeholders for data to be fed in\n",
    "            \n",
    "            self.inputs_ = tf.placeholder(dtype=tf.float32, \n",
    "                                          shape=[None, *STATE_SIZE], \n",
    "                                          # *STATE_SIZE means we take each element of STATE_SIZE and input sequentially\n",
    "                                          # ie. [None, 110, 84, 4] instead of [None, [110, 84, 4]]\n",
    "                                          name=\"inputs\")\n",
    "            \n",
    "            self.inputs_shape = self.inputs_.get_shape()\n",
    "            \n",
    "            self.actions_ = tf.placeholder(dtype=tf.float32,\n",
    "                                           shape=[None, ACTION_SIZE],\n",
    "                                           name=\"actions\")\n",
    "            \n",
    "            # Target QVal = Reward (state | actions) + discount * QVal (new_state | total_possible_actions)\n",
    "            self.target_q = tf.placeholder(dtype=tf.float32,\n",
    "                                           shape=[None],\n",
    "                                           name=\"target_q\")\n",
    "            \n",
    "            # First Convolution\n",
    "            self.conv1 = tf.layers.conv2d(inputs=self.inputs_,\n",
    "                                          filters=32,\n",
    "                                          kernel_size=[8,8],\n",
    "                                          strides=[4,4],\n",
    "                                          padding=\"VALID\",\n",
    "                                          kernel_initializer=tf.contrib.layers.xavier_initializer_conv2d(),\n",
    "                                          name = \"conv1\")\n",
    "            \n",
    "            # Output size = (Wâˆ’K+2P)/S+1\n",
    "            # height = (108 - 8 + 2*0) / 4 + 1= 26\n",
    "            # width = (84 - 8) / 4 + 1= 20\n",
    "            \n",
    "            self.conv1_shape = self.conv1.get_shape()\n",
    "            \n",
    "            self.conv1_out = tf.nn.elu(self.conv1, name=\"conv1_out\") # shape = (batch_size, 26 ht, 20 wd, 32 filters)\n",
    "            \n",
    "            # Second Convolution\n",
    "            self.conv2 = tf.layers.conv2d(inputs=self.conv1_out,\n",
    "                                          filters=64,\n",
    "                                          kernel_size=[4,4],\n",
    "                                          strides=[2,2],\n",
    "                                          padding=\"VALID\",\n",
    "                                          kernel_initializer=tf.contrib.layers.xavier_initializer_conv2d(),\n",
    "                                          name = \"conv2\")\n",
    "            \n",
    "            self.conv2_shape = self.conv2.get_shape()\n",
    "            \n",
    "            self.conv2_out = tf.nn.elu(self.conv2, name=\"conv2_out\") # shape = (batch_size, 12 ht, 9 wd, 64 filters)\n",
    "            \n",
    "            # Third Convolution\n",
    "            self.conv3 = tf.layers.conv2d(inputs=self.conv2_out,\n",
    "                                          filters=64,\n",
    "                                          kernel_size=[3,3],\n",
    "                                          strides=[2,2],\n",
    "                                          padding=\"VALID\",\n",
    "                                          kernel_initializer=tf.contrib.layers.xavier_initializer_conv2d(),\n",
    "                                          name = \"conv3\")\n",
    "            \n",
    "            self.conv3_shape = self.conv2.get_shape()\n",
    "            \n",
    "            self.conv3_out = tf.nn.elu(self.conv3, name=\"conv3_out\") # shape = (batch size, 12 ht, 9 wd, 64 filters)\n",
    "            \n",
    "            # Flat\n",
    "            self.flat = tf.layers.flatten(self.conv3_out)\n",
    "            \n",
    "            self.flat_shape = self.flat.get_shape()\n",
    "            \n",
    "            # Hidden Layer\n",
    "            self.fully_connected = tf.layers.dense(inputs=self.flat,\n",
    "                                                   units=HIDDEN_UNITS,\n",
    "                                                   activation=tf.nn.relu,\n",
    "                                                   kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                                   name=\"fc1\")\n",
    "            \n",
    "            self.fully_connected_shape = self.fully_connected.get_shape()\n",
    "            \n",
    "            # Linear\n",
    "            self.output = tf.layers.dense(inputs=self.fully_connected,\n",
    "                                          units=ACTION_SIZE)\n",
    "            \n",
    "            self.output_shape = self.output.get_shape()\n",
    "            \n",
    "            # q val\n",
    "            self.q = tf.reduce_sum(tf.multiply(self.output, self.actions_))\n",
    "        \n",
    "            # loss = (discounted_q_vals_in_future - q_val_of_move)^2\n",
    "            self.loss = tf.reduce_mean(tf.square(self.target_q - self.q))\n",
    "            \n",
    "            # backprop\n",
    "            self.optimizer = tf.train.AdamOptimizer(LEARNING_RATE).minimize(self.loss)                    \n",
    "            \n",
    "    def print_shapes(self):\n",
    "        print(\"inputs \", self.inputs_shape)\n",
    "        print(\"c1 \", self.conv1_shape)\n",
    "        print(\"c2 \", self.conv2_shape)\n",
    "        print(\"c3 \", self.conv3_shape)\n",
    "        print(\"flat \", self.fully_connected_shape)\n",
    "        print(\"fully_connected \", self.fully_connected_shape)\n",
    "        print(\"output \", self.output_shape)\n",
    "            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
