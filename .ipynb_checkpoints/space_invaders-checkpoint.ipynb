{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import tensorflow as tf\n",
    "import retro\n",
    "\n",
    "import numpy as np\n",
    "import random as r\n",
    "\n",
    "from skimage import transform # to preprocess game frames\n",
    "from skimage.color import rgb2gray # to convert game frames into grayscale\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from collections import deque # to create ordered collections of frames\n",
    "\n",
    "import warnings # ignore warning messages from skiimage during training\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frame Size:  Box(210, 160, 3)\n",
      "Action Space:  6  moves\n",
      "[[1 0 0 0 0 0]\n",
      " [0 1 0 0 0 0]\n",
      " [0 0 1 0 0 0]\n",
      " [0 0 0 1 0 0]\n",
      " [0 0 0 0 1 0]\n",
      " [0 0 0 0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('SpaceInvaders-v0') # make env\n",
    "\n",
    "print(\"Frame Size: \", env.observation_space)\n",
    "print(\"Action Space: \", env.action_space.n, \" moves\")\n",
    "\n",
    "H = 110 # height of preprocessed image\n",
    "W = 84 # width of preprocessed image\n",
    "\n",
    "moves = np.array(np.identity(env.action_space.n, dtype=int).tolist())\n",
    "\n",
    "print(moves)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input size (210, 160, 3)\n"
     ]
    }
   ],
   "source": [
    "for i_episode in range(1):\n",
    "    observation = env.reset()\n",
    "    print(\"input size\", observation.shape)\n",
    "#     for t in range(1):\n",
    "#         env.render()\n",
    "#         print(observation)\n",
    "#         action = env.action_space.sample()\n",
    "#         observation, reward, done, info = env.step(action)\n",
    "#         if done:\n",
    "#             print(\"Episode finished after {} timesteps\".format(t+1))\n",
    "#             break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "STATE_SIZE = [110, 84, 4] # stack dimensions\n",
    "STACK_SIZE = 4 # \"phi length\" - how many frames we consolidate into one training example\n",
    "ACTION_SIZE = env.action_space.n # 6 possible moves\n",
    "LEARNING_RATE = 2.5e-4\n",
    "\n",
    "# training\n",
    "NUM_GAMES = 1 # how many games we will play for training\n",
    "MAX_STEPS = 50000 # max number of moves we'll make each game\n",
    "HIDDEN_UNITS = 512\n",
    "BATCH_SIZE = 64 # num of states we include for each weight update in gradient descent\n",
    "TRAINING = True # when you just want agent to play\n",
    "ENV_RENDER = False # when you want to see env\n",
    "\n",
    "# explore params\n",
    "EXP_START = 1.0\n",
    "EXP_STOP = 0.01\n",
    "DECAY = 1e-5\n",
    "\n",
    "# q le table\n",
    "GAMMA = 0.001 # discount for future rewards in bellman equation\n",
    "\n",
    "# memory\n",
    "LOAD_MEM = BATCH_SIZE # num experiences to be stored in memory during initialization\n",
    "MEMORY_SIZE = int(1e6) # max num experiences stored in memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess raw input from game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def state_to_frame(state):\n",
    "    \n",
    "#     print(\"Input Size: \", state.shape)\n",
    "    \n",
    "    gray_doe = rgb2gray(state)\n",
    "    crop_doe = gray_doe[6:-12, 4:-12]\n",
    "    normalize_doe = crop_doe / 255.0\n",
    "    frame = transform.resize(normalize_doe, [H, W])\n",
    "    \n",
    "#     print(\"Output Size: \", frame.shape)\n",
    "    \n",
    "    return  frame # Shape = 108 height, 84 width | vals = [0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stack_frames(stack_o_frames, state, is_new_game):\n",
    "    \n",
    "    frame = state_to_frame(state)\n",
    "    \n",
    "    if is_new_game: # if new game, clear stack from last game\n",
    "        \n",
    "        for i in range(STACK_SIZE): # in new game, start with copies of OG frame\n",
    "            stack_o_frames.append(frame)\n",
    "            \n",
    "#         print(\"Stack Size: \", len(stack_o_frames))  \n",
    "#         print(\"State Size: \", state.shape)\n",
    "        \n",
    "    else: # if we already have stack, append latest frame\n",
    "        \n",
    "        stack_o_frames.append(frame) # deque automatically removes oldest frame\n",
    "        \n",
    "    state = np.stack(stack_o_frames, axis=2) # input to neural net, shape = (108, 84, 4)\n",
    "       \n",
    "#     print(\"Snapshot: \", state[:5])\n",
    "        \n",
    "    return stack_o_frames, state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN:\n",
    "    def __init__(self, name=\"DQN\"):\n",
    "        \n",
    "        with tf.variable_scope(name): # create model skeleton, using placeholders for data to be fed in\n",
    "            \n",
    "            self.inputs_ = tf.placeholder(dtype=tf.float32, \n",
    "                                          shape=[None, *STATE_SIZE], \n",
    "                                          # *STATE_SIZE means we take each element of STATE_SIZE and input sequentially\n",
    "                                          # ie. [None, 110, 84, 4] instead of [None, [110, 84, 4]]\n",
    "                                          name=\"inputs\")\n",
    "            \n",
    "            self.actions_ = tf.placeholder(dtype=tf.float32,\n",
    "                                           shape=[None, ACTION_SIZE],\n",
    "                                           name=\"actions\")\n",
    "            \n",
    "            # Target QVal = Reward (state | actions) + discount * QVal (new_state | total_possible_actions)\n",
    "            self.target_q = tf.placeholder(dtype=tf.float32,\n",
    "                                           shape=[None],\n",
    "                                           name=\"target_q\")\n",
    "            \n",
    "            # First Convolution\n",
    "            self.conv1 = tf.layers.conv2d(inputs=self.inputs_,\n",
    "                                          filters=32,\n",
    "                                          kernel_size=[8,8],\n",
    "                                          strides=[4,4],\n",
    "                                          padding=\"VALID\",\n",
    "                                          kernel_initializer=tf.contrib.layers.xavier_initializer_conv2d(),\n",
    "                                          name = \"conv1\")\n",
    "            \n",
    "            # Output size = (Wâˆ’K+2P)/S+1\n",
    "            # height = (108 - 8 + 2*0) / 4 + 1= 26\n",
    "            # width = (84 - 8) / 4 + 1= 20\n",
    "            \n",
    "            self.conv1_shape = self.conv1.get_shape()\n",
    "            \n",
    "            self.conv1_out = tf.nn.elu(self.conv1, name=\"conv1_out\") # shape = (batch_size, 26 ht, 20 wd, 32 filters)\n",
    "            \n",
    "            # Second Convolution\n",
    "            self.conv2 = tf.layers.conv2d(inputs=self.conv1_out,\n",
    "                                          filters=64,\n",
    "                                          kernel_size=[4,4],\n",
    "                                          strides=[2,2],\n",
    "                                          padding=\"VALID\",\n",
    "                                          kernel_initializer=tf.contrib.layers.xavier_initializer_conv2d(),\n",
    "                                          name = \"conv2\")\n",
    "            \n",
    "            self.conv2_shape = self.conv2.get_shape()\n",
    "            \n",
    "            self.conv2_out = tf.nn.elu(self.conv2, name=\"conv2_out\") # shape = (batch_size, 12 ht, 9 wd, 64 filters)\n",
    "            \n",
    "            # Third Convolution\n",
    "            self.conv3 = tf.layers.conv2d(inputs=self.conv2_out,\n",
    "                                          filters=64,\n",
    "                                          kernel_size=[3,3],\n",
    "                                          strides=[2,2],\n",
    "                                          padding=\"VALID\",\n",
    "                                          kernel_initializer=tf.contrib.layers.xavier_initializer_conv2d(),\n",
    "                                          name = \"conv3\")\n",
    "            \n",
    "            self.conv3_shape = self.conv2.get_shape()\n",
    "            \n",
    "            self.conv3_out = tf.nn.elu(self.conv3, name=\"conv3_out\") # shape = (batch size, 12 ht, 9 wd, 64 filters)\n",
    "            \n",
    "            # Flat\n",
    "            self.flat = tf.layers.flatten(self.conv3_out)\n",
    "            \n",
    "            self.flat_shape = self.flat.get_shape()\n",
    "            \n",
    "            # Hidden Layer\n",
    "            self.fully_connected = tf.layers.dense(inputs=self.flat,\n",
    "                                                   units=HIDDEN_UNITS,\n",
    "                                                   activation=tf.nn.elu,\n",
    "                                                   kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                                   name=\"fc1\")\n",
    "            \n",
    "            self.fully_connected_shape = self.fully_connected.get_shape()\n",
    "            \n",
    "            # Linear\n",
    "            self.output = tf.layers.dense(inputs=self.fully_connected,\n",
    "                                          units=ACTION_SIZE,\n",
    "                                          kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                          activation=None)\n",
    "            \n",
    "            self.output_shape = self.output.get_shape()\n",
    "            \n",
    "            # q val\n",
    "            self.q = tf.reduce_sum(tf.multiply(self.output, self.actions_))\n",
    "        \n",
    "            # loss = (discounted_q_vals_in_future - q_val_of_move)^2\n",
    "            self.loss = tf.reduce_mean(tf.square(self.target_q - self.q))\n",
    "            \n",
    "            # backprop\n",
    "            self.optimizer = tf.train.AdamOptimizer(LEARNING_RATE).minimize(self.loss)                    \n",
    "            \n",
    "    def print_shapes(self):\n",
    "        print(\"c1 \", self.conv1_shape)\n",
    "        print(\"c2 \", self.conv2_shape)\n",
    "        print(\"c3 \", self.conv3_shape)\n",
    "        print(\"flat \", self.fully_connected_shape)\n",
    "        print(\"fully_connected \", self.fully_connected_shape)\n",
    "        print(\"output \", self.output_shape)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From <ipython-input-7-1c3f4fc01f22>:28: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.conv2d instead.\n",
      "WARNING:tensorflow:From /Users/katz/Applications/anaconda3/envs/rl/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From <ipython-input-7-1c3f4fc01f22>:65: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.flatten instead.\n",
      "WARNING:tensorflow:From <ipython-input-7-1c3f4fc01f22>:74: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dense instead.\n",
      "c1  (?, 26, 20, 32)\n",
      "c2  (?, 12, 9, 64)\n",
      "c3  (?, 12, 9, 64)\n",
      "flat  (?, 512)\n",
      "fully_connected  (?, 512)\n",
      "output  (?, 6)\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "DQN = DQN()\n",
    "\n",
    "DQN.print_shapes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Memory module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Memory():\n",
    "    def __init__(self):\n",
    "        self.buffer = deque(maxlen = MEMORY_SIZE)\n",
    "        \n",
    "    def add(self, experience):\n",
    "        self.buffer.append(experience)\n",
    "        \n",
    "    def sample(self):\n",
    "        buffer_size = len(self.buffer)\n",
    "        index = np.random.choice(np.arange(buffer_size),\n",
    "                                 size=BATCH_SIZE,\n",
    "                                 replace=False)\n",
    "        \n",
    "        return [self.buffer[i] for i in index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiences Loaded:  64\n"
     ]
    }
   ],
   "source": [
    "memory = Memory()\n",
    "\n",
    "state = env.reset() # reset game\n",
    "stack = deque([np.zeros((H,W), dtype=np.int) for i in range(STACK_SIZE)], maxlen=STACK_SIZE) # load deque with zeros\n",
    "\n",
    "# load first state, create first stack\n",
    "stack, state = stack_frames(stack, state, is_new_game=True)\n",
    "\n",
    "for i in range(LOAD_MEM):\n",
    "    \n",
    "    choice = r.randint(1, len(moves)) - 1 # random index\n",
    "    action = moves[choice] # chosen index\n",
    "    next_state, reward, done, _logs_ = env.step(choice) # act\n",
    "    \n",
    "    env.render()\n",
    "    \n",
    "    stack, next_state = stack_frames(stack, next_state, is_new_game=False) # save next state\n",
    "    \n",
    "    if done: # when we lose, create a 0-matrix for next_state\n",
    "        next_state = np.zeros(state.shape)\n",
    "    \n",
    "    memory.add((state, action, reward, next_state, done)) # add experience to memory \n",
    "    \n",
    "    state = next_state\n",
    "    \n",
    "    if done: # start new game\n",
    "        state = env.reset()\n",
    "        stack, state = stack_frames(stack, state, is_new_game=True) \n",
    "\n",
    "print(\"Experiences Loaded: \", len(memory.buffer))\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = tf.summary.FileWriter(\"./tensorboard/dqn/1\") # write\n",
    "\n",
    "tf.summary.scalar(\"Loss\", DQN.loss) \n",
    "\n",
    "write_op = tf.summary.merge_all() # merge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def move(state, moves, training_step):\n",
    "    \n",
    "    adventure = r.uniform(0, 1)\n",
    "\n",
    "    explore_prob = EXP_STOP - (EXP_START - EXP_STOP) * np.exp(-DECAY * training_step)\n",
    "\n",
    "    if explore_prob > adventure: # explore randomly\n",
    "        choice = r.randint(1, len(moves)) - 1\n",
    "        action = moves[choice]\n",
    "        \n",
    "    else: # use dqn\n",
    "        dq_table = sesh.run(DQN.output, feed_dict={DQN.inputs_:state.reshape((1, *state.shape))}) # run graph with state as input\n",
    "        choice = np.argmax(dq_table) # pick move with highest predicted q val\n",
    "        action = moves[choice]\n",
    "        \n",
    "    return action, explore_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn(DQN, memory, sesh):\n",
    "\n",
    "    batch = memory.sample()\n",
    "\n",
    "    # retrieve minibatches from memory\n",
    "    states_mini = np.array([states[0] for states in batch], ndmin=3)\n",
    "    actions_mini = np.array([actions[1] for actions in batch])\n",
    "    rewards_mini = np.array([rewards[2] for rewards in batch])\n",
    "    next_states_mini = np.array([next_states[3] for next_states in batch])\n",
    "    dones_mini = np.array([dones[4] for dones in batch])\n",
    "    \n",
    "    target_qs = []\n",
    "    \n",
    "    # predict q vals for next state\n",
    "    qvals_next_state = sesh.run(DQN.output, feed_dict={DQN.inputs_:next_states_mini})\n",
    "    \n",
    "    for i in range(0, len(batch)): # loop through batches\n",
    "        \n",
    "        gameover = dones_mini[i]\n",
    "        \n",
    "        if gameover: # target_q = reward\n",
    "            target_qs.append(rewards_mini[i])\n",
    "        \n",
    "        else: # target_q = reward + gamma * max_q(s', a')\n",
    "            target = rewards_mini[i] + GAMMA * np.max(qvals_next_state[i])\n",
    "            target_qs.append(target)\n",
    "            \n",
    "    targets_mini = np.array([target for target in target_qs_batch]) # create minibatch of target q vals\n",
    "    \n",
    "    # calculate loss and backprop on minibatches\n",
    "    loss, _ = sesh.run([DQN.loss, DQN.optimizer], feed_dict={DQN.inputs_:states_mini,\n",
    "                                                             DQN.target_q:targets_mini,\n",
    "                                                             DQN.actions_:actions_mini})\n",
    "    \n",
    "    # wrte to tensorboard during each minibatch\n",
    "    summary = sesh.run(write_op, feed_dict={DQN.inputs_:states_mini,\n",
    "                                            DQN.target_q:targets_mini,\n",
    "                                            DQN.actions_:actions_mini})\n",
    "    writer.add_summary(summarym, game)\n",
    "    writer.flush()\n",
    "    \n",
    "    if game % 5 == 0: # save model every five games\n",
    "        save_path = saver.save('./models/model.cpkt')\n",
    "        print('Checkpoint at Game {}'.format(game))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-f58440d383fb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     52\u001b[0m                     \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m                 \u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDQN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msesh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-24-bd225db37517>\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(DQN, memory, sesh)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;31m# predict q vals for next state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mqvals_next_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msesh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDQN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mDQN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs_\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mnext_states_mini\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# loop through batches\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Applications/anaconda3/envs/rl/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Applications/anaconda3/envs/rl/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1119\u001b[0m             \u001b[0mfeed_handles\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msubfeed_t\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msubfeed_val\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1120\u001b[0m           \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1121\u001b[0;31m             \u001b[0mnp_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubfeed_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msubfeed_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m           if (not is_tensor_handle_feed and\n",
      "\u001b[0;32m~/Applications/anaconda3/envs/rl/lib/python3.7/site-packages/numpy/core/numeric.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m    536\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    537\u001b[0m     \"\"\"\n\u001b[0;32m--> 538\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    539\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence."
     ]
    }
   ],
   "source": [
    "saver = tf.train.Saver() \n",
    "\n",
    "if TRAINING == True:\n",
    "    \n",
    "    with tf.Session() as sesh:\n",
    "        \n",
    "        tf.initialize_all_variables().run()\n",
    "        \n",
    "        training_step = 0\n",
    "        \n",
    "        for game in range(NUM_GAMES): # loop through games\n",
    "            \n",
    "            # initialize\n",
    "            game_step = 0\n",
    "            game_rewards = []\n",
    "            state = env.reset()\n",
    "            \n",
    "            stack, state = stack_frames(stack, state, is_new_game=True)\n",
    "            \n",
    "            while game_step < MAX_STEPS:\n",
    "                \n",
    "                game_step += 1\n",
    "                training_step += 1\n",
    "                \n",
    "                action, explore_prob = move(state, moves, training_step) # choose move\n",
    "                \n",
    "                action = np.argmax(action)\n",
    "                \n",
    "                next_state, reward, done, _logs_ = env.step(action) # act\n",
    "                \n",
    "                if ENV_RENDER: # show games\n",
    "                    env.render()\n",
    "                    \n",
    "                game_rewards.append(reward) # track reward\n",
    "                \n",
    "                if done: # we've lost\n",
    "                    next_state = np.zeros(state.shape)\n",
    "                    next_state, stack = stack_frames(stack, next_state, is_new_game=False) \n",
    "                    step = MAX_STEPS\n",
    "                    total_reward = np.sum(game_rewards)\n",
    "                    rewards_list.append(total_reward)\n",
    "                    memory.add((state, action, reward, next_state, done))\n",
    "                    \n",
    "                    print('Episode: {}'.format(episode), \n",
    "                          'Total reward: {}'.format(total_reward),\n",
    "                          'Explore P: {:.4f}'.format(explore_probability),\n",
    "                          'Training Loss {:.4f}'.format(loss))\n",
    "        \n",
    "                else: # we're still alive!\n",
    "                    next_state, stack, stack_frames(stack, next_state, is_new_game=False)\n",
    "                    memory.add((state, action, reward, next_state, done))\n",
    "                    state = next_state\n",
    "                \n",
    "                learn(DQN, memory, sesh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
